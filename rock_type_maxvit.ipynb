{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ebd5bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 메모리 사용량:\n",
      "사용 중: 0.00 MB\n",
      "예약 중: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU 메모리 사용량:\")\n",
    "    print(f\"사용 중: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "    print(f\"예약 중: {torch.cuda.memory_reserved() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0505fba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/metaai2/miniconda3/envs/env312_cuda124_torch260/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: maxvit_xlarge_tf_384.in21k_ft_in1k\n",
      "Model classifier replaced for 7 classes.\n",
      "Using timm's default validation transform for maxvit_xlarge_tf_384.in21k_ft_in1k.\n",
      "Timm default input size for maxvit_xlarge_tf_384.in21k_ft_in1k: (3, 384, 384)\n",
      "Timm default mean: (0.5, 0.5, 0.5), std: (0.5, 0.5, 0.5)\n",
      "Overriding to use specified IMG_SIZE: 384 for validation transform consistency.\n",
      "Loading datasets from: /home/metaai2/workspace/limseunghwan/open/train and /home/metaai2/workspace/limseunghwan/open/val\n",
      "Found 342015 training images and 38005 validation images.\n",
      "Classes: ['Andesite', 'Basalt', 'Etc', 'Gneiss', 'Granite', 'Mud_Sandstone', 'Weathered_Rock']\n",
      "Calculated Focal Loss alpha (normalized): [0.36379087 0.59434706 1.         0.21558282 0.17148152 0.17810482\n",
      " 0.4287038 ]\n",
      "Using Focal Loss with calculated alpha.\n",
      "\n",
      "Starting training process with maxvit_xlarge_tf_384.in21k_ft_in1k...\n",
      "Batch Size: 2. If you encounter OOM errors, try reducing it further.\n",
      "학습 시작: 총 20 에폭, Device: cuda\n",
      "Top-5 모델 저장 디렉토리: ./saved_models_maxvit_xlarge_384\n",
      "평가 기준: Validation Macro F1 Score\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 338\u001b[39m\n\u001b[32m    334\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBatch Size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBATCH_SIZE\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. If you encounter OOM errors, try reducing it further.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    336\u001b[39m model_name_base = MODEL_NAME.split(\u001b[33m'\u001b[39m\u001b[33m/\u001b[39m\u001b[33m'\u001b[39m)[-\u001b[32m1\u001b[39m].split(\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m)[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33m/\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m MODEL_NAME \u001b[38;5;28;01melse\u001b[39;00m MODEL_NAME.split(\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m)[\u001b[32m0\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m history = \u001b[43mtrain_and_validate_best_f1\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    339\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    340\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    342\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    343\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    344\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    345\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    346\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mNUM_CLASSES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mSAVE_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name_base\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_name_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgradient_clipping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mGRADIENT_CLIPPING\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwarmup_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mWARMUP_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbase_lr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBASE_LR\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTraining finished.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m history[\u001b[33m'\u001b[39m\u001b[33mval_macro_f1_scores\u001b[39m\u001b[33m'\u001b[39m]:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 114\u001b[39m, in \u001b[36mtrain_and_validate_best_f1\u001b[39m\u001b[34m(model, train_loader, val_loader, optimizer, criterion, epochs, device, num_classes, save_dir, model_name_base, top_k, gradient_clipping, lr_scheduler, warmup_epochs, base_lr)\u001b[39m\n\u001b[32m    112\u001b[39m         torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clipping)\n\u001b[32m    113\u001b[39m     optimizer.step()\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m     running_train_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    115\u001b[39m     train_pbar.set_postfix(loss=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss.item()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    116\u001b[39m epoch_train_loss = running_train_loss / \u001b[38;5;28mlen\u001b[39m(train_loader)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6127f2e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/metaai2/miniconda3/envs/env312_cuda124_torch260/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: maxvit_xlarge_tf_384.in21k_ft_in1k\n",
      "Model classifier adapted for 7 classes.\n",
      "Using timm's default validation transform config for maxvit_xlarge_tf_384.in21k_ft_in1k.\n",
      "Timm default input size: (3, 384, 384)\n",
      "Timm default mean: (0.5, 0.5, 0.5), std: (0.5, 0.5, 0.5)\n",
      "Loading datasets from: /home/metaai2/workspace/limseunghwan/open/train and /home/metaai2/workspace/limseunghwan/open/val\n",
      "Found 342015 training images and 38005 validation images.\n",
      "Using predefined CLASS_NAMES: ['Andesite', 'Basalt', 'Etc', 'Gneiss', 'Granite', 'Mud_Sandstone', 'Weathered_Rock']\n",
      "Calculated Focal Loss alpha (normalized): [0.36379087 0.59434706 1.         0.21558282 0.17148152 0.17810482\n",
      " 0.4287038 ]\n",
      "Using Focal Loss with calculated alpha.\n",
      "\n",
      "Starting training process with maxvit_xlarge_tf_384.in21k_ft_in1k...\n",
      "Batch Size: 2. If OOM errors, reduce it.\n",
      "학습 시작: 총 50 에폭, Device: cuda\n",
      "Top-5 모델 저장 디렉토리: ./saved_models_maxvit_xlarge_tf_384_384\n",
      "평가 기준: Validation Macro F1 Score\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 522\u001b[39m\n\u001b[32m    519\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mStarting training process with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMODEL_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    520\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBatch Size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBATCH_SIZE\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. If OOM errors, reduce it.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m522\u001b[39m history = \u001b[43mtrain_and_validate_best_f1\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Use the globally defined model for training\u001b[39;49;00m\n\u001b[32m    524\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    525\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    526\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    527\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    528\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    529\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    530\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mNUM_CLASSES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    531\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mSAVE_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    532\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name_base\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMODEL_ARTIFACT_NAME_BASE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    533\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    534\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgradient_clipping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mGRADIENT_CLIPPING\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    536\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwarmup_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mWARMUP_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    537\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbase_lr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBASE_LR\u001b[49m\n\u001b[32m    538\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    540\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTraining finished.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    541\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m history[\u001b[33m'\u001b[39m\u001b[33mval_macro_f1_scores\u001b[39m\u001b[33m'\u001b[39m]:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 116\u001b[39m, in \u001b[36mtrain_and_validate_best_f1\u001b[39m\u001b[34m(model, train_loader, val_loader, optimizer, criterion, epochs, device, num_classes, save_dir, model_name_base, top_k, gradient_clipping, lr_scheduler, warmup_epochs, base_lr)\u001b[39m\n\u001b[32m    114\u001b[39m         torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clipping)\n\u001b[32m    115\u001b[39m     optimizer.step()\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m     running_train_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    117\u001b[39m     train_pbar.set_postfix(loss=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss.item()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    118\u001b[39m epoch_train_loss = running_train_loss / \u001b[38;5;28mlen\u001b[39m(train_loader)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# MaxViT.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset # Added Dataset for TestImageDataset\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "import torchmetrics\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd # For inference CSV handling\n",
    "from PIL import Image # For inference image loading\n",
    "\n",
    "# --- 1. Focal Loss 클래스 정의 (이전 코드에서 복사) ---\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2., reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            if isinstance(self.alpha, (float, int)):\n",
    "                 alpha = torch.tensor([self.alpha] * inputs.shape[1], device=inputs.device)\n",
    "            elif isinstance(self.alpha, list):\n",
    "                 alpha = torch.tensor(self.alpha, device=inputs.device, dtype=torch.float32)\n",
    "            elif torch.is_tensor(self.alpha):\n",
    "                 alpha = self.alpha.to(device=inputs.device, dtype=torch.float32)\n",
    "            else:\n",
    "                 raise TypeError(\"alpha must be float, list or torch.Tensor\")\n",
    "\n",
    "            if alpha.shape[0] != inputs.shape[1]:\n",
    "                 raise ValueError(f\"alpha size {alpha.shape[0]} does not match C {inputs.shape[1]}\")\n",
    "\n",
    "            alpha_t = alpha.gather(0, targets)\n",
    "            focal_loss = alpha_t * focal_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else: # 'none'\n",
    "            return focal_loss\n",
    "\n",
    "# --- 2. 학습 및 검증 함수 정의 (이전 코드에서 복사) ---\n",
    "def train_and_validate_best_f1(model: nn.Module,\n",
    "                               train_loader: DataLoader,\n",
    "                               val_loader: DataLoader,\n",
    "                               optimizer: optim.Optimizer,\n",
    "                               criterion: nn.Module,\n",
    "                               epochs: int,\n",
    "                               device: torch.device,\n",
    "                               num_classes: int,\n",
    "                               save_dir: str,\n",
    "                               model_name_base: str,\n",
    "                               top_k: int = 5,\n",
    "                               gradient_clipping: float = None,\n",
    "                               lr_scheduler = None,\n",
    "                               warmup_epochs: int = 0,\n",
    "                               base_lr: float = 1e-5\n",
    "                              ):\n",
    "\n",
    "    history = {'train_losses': [], 'val_losses': [], 'val_macro_f1_scores': [], 'best_model_path': None} # Added best_model_path\n",
    "    max_val_f1 = 0.0\n",
    "    best_epoch = -1\n",
    "    top_k_checkpoints = []\n",
    "\n",
    "    f1_metric = torchmetrics.F1Score(task='multiclass', num_classes=num_classes, average='macro').to(device)\n",
    "    patience = 10 # 조기 종료를 위한 인내 에폭 수 (조금 늘려봄)\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"학습 시작: 총 {epochs} 에폭, Device: {device}\")\n",
    "    print(f\"Top-{top_k} 모델 저장 디렉토리: {save_dir}\")\n",
    "    print(f\"평가 기준: Validation Macro F1 Score\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "        if epoch < warmup_epochs:\n",
    "            warmup_factor = (epoch + 1) / warmup_epochs\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = base_lr * warmup_factor\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "        elif lr_scheduler is not None:\n",
    "             if epoch == warmup_epochs:\n",
    "                 for param_group in optimizer.param_groups:\n",
    "                     param_group['lr'] = base_lr\n",
    "                 current_lr = optimizer.param_groups[0]['lr']\n",
    "             lr_scheduler.step()\n",
    "             current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "        train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Train] LR: {current_lr:.1e}\", leave=False)\n",
    "        for images, labels in train_pbar:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            if gradient_clipping is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clipping)\n",
    "            optimizer.step()\n",
    "            running_train_loss += loss.item()\n",
    "            train_pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "        epoch_train_loss = running_train_loss / len(train_loader)\n",
    "        history['train_losses'].append(epoch_train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        running_val_loss = 0.0\n",
    "        f1_metric.reset()\n",
    "        val_pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} [Val] \", leave=False)\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_pbar:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                running_val_loss += loss.item()\n",
    "                f1_metric.update(outputs, labels)\n",
    "                val_pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "        epoch_val_loss = running_val_loss / len(val_loader)\n",
    "        history['val_losses'].append(epoch_val_loss)\n",
    "        epoch_val_f1 = f1_metric.compute().item()\n",
    "        history['val_macro_f1_scores'].append(epoch_val_f1)\n",
    "\n",
    "        epoch_end_time = time.time()\n",
    "        epoch_duration = epoch_end_time - epoch_start_time\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] ({epoch_duration:.2f}s) - \"\n",
    "              f\"Train Loss: {epoch_train_loss:.4f}, \"\n",
    "              f\"Val Loss: {epoch_val_loss:.4f}, \"\n",
    "              f\"Val Macro F1: {epoch_val_f1:.4f}\")\n",
    "\n",
    "        is_top_k = len(top_k_checkpoints) < top_k or epoch_val_f1 > top_k_checkpoints[-1][0]\n",
    "\n",
    "        if is_top_k:\n",
    "            checkpoint_filename = f\"{model_name_base}_epoch{epoch+1:03d}_f1_{epoch_val_f1:.4f}.pth\" #epoch zero-padding\n",
    "            checkpoint_path = os.path.join(save_dir, checkpoint_filename)\n",
    "            try:\n",
    "                torch.save(model.state_dict(), checkpoint_path)\n",
    "                print(f\"  Checkpoint saved to {checkpoint_path}\")\n",
    "                top_k_checkpoints.append((epoch_val_f1, checkpoint_path))\n",
    "                top_k_checkpoints.sort(key=lambda x: x[0], reverse=True)\n",
    "                if len(top_k_checkpoints) > top_k:\n",
    "                    score_to_remove, path_to_remove = top_k_checkpoints.pop()\n",
    "                    print(f\"  Removing checkpoint {os.path.basename(path_to_remove)} (score: {score_to_remove:.4f}) as it's no longer in top-{top_k}\")\n",
    "                    if os.path.exists(path_to_remove):\n",
    "                        try:\n",
    "                            os.remove(path_to_remove)\n",
    "                        except Exception as e_rem:\n",
    "                            print(f\"    Error removing file {path_to_remove}: {e_rem}\")\n",
    "                    else:\n",
    "                        print(f\"    Warning: File to remove not found: {path_to_remove}\")\n",
    "            except Exception as e_save:\n",
    "                print(f\"  Error saving checkpoint: {e_save}\")\n",
    "\n",
    "        if epoch_val_f1 > max_val_f1:\n",
    "            print(f\"  Validation Macro F1 improved ({max_val_f1:.4f} --> {epoch_val_f1:.4f}).\")\n",
    "            max_val_f1 = epoch_val_f1\n",
    "            best_epoch = epoch\n",
    "            if top_k_checkpoints: # Best model is the first in sorted top_k\n",
    "                history['best_model_path'] = top_k_checkpoints[0][1]\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            print(f\"  Validation Macro F1 did not improve from the best ({max_val_f1:.4f}). ({epochs_no_improve}/{patience})\")\n",
    "\n",
    "        if epochs_no_improve >= patience and epoch >= warmup_epochs + patience : # Ensure warmup isn't cut short, and some training happened\n",
    "            print(f\"Early stopping triggered at epoch {epoch+1} after {patience} epochs without improvement from the best F1 score.\")\n",
    "            break\n",
    "\n",
    "    print(f\"\\n학습 완료.\")\n",
    "    print(f\"최종 Top-{top_k} 모델 성능 및 경로:\")\n",
    "    if not top_k_checkpoints:\n",
    "        print(\"  No models were saved.\")\n",
    "    else:\n",
    "        for i, (score, path) in enumerate(top_k_checkpoints):\n",
    "            print(f\"  Top {i+1}: Score={score:.4f}, Path={path}\")\n",
    "        if not history['best_model_path'] and top_k_checkpoints: # Fallback if not set during improvement\n",
    "             history['best_model_path'] = top_k_checkpoints[0][1]\n",
    "\n",
    "    print(f\"\\nOverall Best Epoch: {best_epoch+1 if best_epoch != -1 else 'N/A'}, Overall Best Validation Macro F1: {max_val_f1:.4f}\")\n",
    "    if history['best_model_path']:\n",
    "        print(f\"Path to best model: {history['best_model_path']}\")\n",
    "    else:\n",
    "        print(\"No best model path recorded (possibly no improvement or no models saved).\")\n",
    "\n",
    "\n",
    "    return history\n",
    "\n",
    "# --- 3. 설정 변수 정의 ---\n",
    "MODEL_NAME = 'maxvit_xlarge_tf_384.in21k_ft_in1k' # MaxViT XLarge\n",
    "NUM_CLASSES = 7\n",
    "IMG_SIZE = 384\n",
    "\n",
    "# --- 데이터 경로 설정 ---\n",
    "# TODO: 실제 데이터 경로로 수정하세요\n",
    "TRAIN_DATA_DIR = r\"/home/metaai2/workspace/limseunghwan/open/train\"\n",
    "VAL_DATA_DIR = r\"/home/metaai2/workspace/limseunghwan/open/val\"\n",
    "# --- 테스트 데이터 및 결과 경로 설정 (추가) ---\n",
    "TEST_CSV_PATH = r'/home/metaai2/workspace/limseunghwan/open/test.csv'    # Path to your test.csv\n",
    "IMAGE_BASE_DIR = r'/home/metaai2/workspace/limseunghwan/open'           # Base directory for images in test.csv\n",
    "# OUTPUT_CSV_PATH will be generated dynamically based on model name and performance\n",
    "\n",
    "# --- 학습 하이퍼파라미터 ---\n",
    "EPOCHS = 50 # 에폭 수 (조절)\n",
    "BATCH_SIZE = 2 # MaxViT-XLarge는 매우 많은 메모리를 사용\n",
    "BASE_LR = 1e-5\n",
    "WEIGHT_DECAY = 1e-2\n",
    "WARMUP_EPOCHS = 5\n",
    "GRADIENT_CLIPPING = 1.0\n",
    "\n",
    "# --- 시스템 설정 ---\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_WORKERS = os.cpu_count() // 2 if os.cpu_count() else 4\n",
    "\n",
    "# --- 저장 경로 설정 ---\n",
    "MODEL_ARTIFACT_NAME_BASE = MODEL_NAME.split('/')[-1].split('.')[0] if '/' in MODEL_NAME else MODEL_NAME.split('.')[0]\n",
    "SAVE_DIR = f'./saved_models_{MODEL_ARTIFACT_NAME_BASE}_{IMG_SIZE}' # 모델 저장 디렉토리\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# --- 클래스 이름 정의 (중요!) ---\n",
    "# TODO: ***매우 중요*** 당신의 데이터셋에 맞는 실제 클래스 이름을 순서대로 정의하세요.\n",
    "#       train_dataset.classes 에서 가져올 수 있습니다.\n",
    "#       예시: CLASS_NAMES = ['andesite', 'gneiss', 'granite', 'mudstone', 'quartzite', 'rhyolite', 'sandstone']\n",
    "CLASS_NAMES = ['Andesite', 'Basalt', 'Etc', 'Gneiss', 'Granite', 'Mud_Sandstone', 'Weathered_Rock'] # 예시 값, 반드시 수정!\n",
    "# CLASS_NAMES = [] # 아래에서 train_dataset.classes로 채우도록 시도합니다.\n",
    "\n",
    "# --- 4. 모델 로드 및 수정 ---\n",
    "print(f\"Loading model: {MODEL_NAME}\")\n",
    "# 모델은 학습 루프 시작 시 또는 추론 시점에 로드/생성됩니다.\n",
    "# 여기서는 전역으로 선언하지 않고, 필요할 때 생성하도록 변경 가능.\n",
    "# 우선은 기존 방식 유지\n",
    "model_train = timm.create_model(MODEL_NAME, pretrained=True, num_classes=NUM_CLASSES, img_size=IMG_SIZE)\n",
    "print(f\"Model classifier adapted for {NUM_CLASSES} classes.\")\n",
    "model_train = model_train.to(DEVICE)\n",
    "\n",
    "\n",
    "# --- 5. 데이터 변환 정의 ---\n",
    "# 학습 변환\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(size=(IMG_SIZE, IMG_SIZE), scale=(0.6, 1.0), ratio=(0.75, 1.3333), interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5), # 추가\n",
    "    transforms.RandomRotation(degrees=30, interpolation=transforms.InterpolationMode.BILINEAR, fill=0), # 각도 증가\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1), # hue jitter 추가\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# 검증/추론 변환 (일관성 유지)\n",
    "try:\n",
    "    # 학습에 사용된 모델과 동일한 구조의 모델 인스턴스를 사용하여 config를 가져옵니다.\n",
    "    # model_train이 이미 생성되어 있으므로 이를 사용합니다.\n",
    "    config = timm.data.resolve_data_config({}, model=model_train)\n",
    "    print(f\"Using timm's default validation transform config for {MODEL_NAME}.\")\n",
    "    print(f\"Timm default input size: {config['input_size']}\")\n",
    "    print(f\"Timm default mean: {config['mean']}, std: {config['std']}\")\n",
    "\n",
    "    val_inference_transform = transforms.Compose([\n",
    "        transforms.Resize(IMG_SIZE, interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "        transforms.CenterCrop(IMG_SIZE),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=config['mean'], std=config['std'])\n",
    "    ])\n",
    "    train_transform.transforms.append(transforms.Normalize(mean=config['mean'], std=config['std']))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Failed to get timm config ({e}), defining transforms manually (using ImageNet defaults).\")\n",
    "    _imagenet_mean = [0.485, 0.456, 0.406]\n",
    "    _imagenet_std = [0.229, 0.224, 0.225]\n",
    "    val_inference_transform = transforms.Compose([\n",
    "        transforms.Resize(IMG_SIZE, interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "        transforms.CenterCrop(IMG_SIZE),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=_imagenet_mean, std=_imagenet_std)\n",
    "    ])\n",
    "    train_transform.transforms.append(transforms.Normalize(mean=_imagenet_mean, std=_imagenet_std))\n",
    "\n",
    "# --- 6. 데이터셋 및 데이터 로더 준비 ---\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "print(f\"Loading datasets from: {TRAIN_DATA_DIR} and {VAL_DATA_DIR}\")\n",
    "try:\n",
    "    train_dataset = ImageFolder(root=TRAIN_DATA_DIR, transform=train_transform)\n",
    "    val_dataset = ImageFolder(root=VAL_DATA_DIR, transform=val_inference_transform) # 검증은 val_inference_transform 사용\n",
    "\n",
    "    print(f\"Found {len(train_dataset)} training images and {len(val_dataset)} validation images.\")\n",
    "    \n",
    "    # CLASS_NAMES 업데이트 시도\n",
    "    if not CLASS_NAMES or len(CLASS_NAMES) != NUM_CLASSES:\n",
    "        print(\"CLASS_NAMES not predefined or mismatched. Attempting to use 'train_dataset.classes'.\")\n",
    "        if hasattr(train_dataset, 'classes') and len(train_dataset.classes) == NUM_CLASSES:\n",
    "            CLASS_NAMES = train_dataset.classes\n",
    "            print(f\"Successfully set CLASS_NAMES from train_dataset: {CLASS_NAMES}\")\n",
    "        else:\n",
    "            print(f\"ERROR: Could not automatically determine CLASS_NAMES. Please define it manually and ensure it matches NUM_CLASSES={NUM_CLASSES}.\")\n",
    "            if hasattr(train_dataset, 'classes'):\n",
    "                print(f\"Found classes in dataset: {train_dataset.classes} (count: {len(train_dataset.classes)})\")\n",
    "            exit()\n",
    "    elif len(CLASS_NAMES) != NUM_CLASSES:\n",
    "        print(f\"ERROR: Predefined CLASS_NAMES length ({len(CLASS_NAMES)}) does not match NUM_CLASSES ({NUM_CLASSES}).\")\n",
    "        exit()\n",
    "    else:\n",
    "        print(f\"Using predefined CLASS_NAMES: {CLASS_NAMES}\")\n",
    "\n",
    "\n",
    "    class_counts = np.bincount([s[1] for s in train_dataset.samples])\n",
    "    focal_loss_alpha = None\n",
    "    if len(class_counts) == NUM_CLASSES:\n",
    "        total_samples = sum(class_counts)\n",
    "        class_weights_raw = [total_samples / count if count > 0 else 0 for count in class_counts]\n",
    "        max_weight = max(class_weights_raw) if any(w > 0 for w in class_weights_raw) else 1\n",
    "        if max_weight > 0:\n",
    "            class_weights_normalized = [w / max_weight for w in class_weights_raw]\n",
    "            focal_loss_alpha = torch.tensor(class_weights_normalized, device=DEVICE, dtype=torch.float32)\n",
    "            print(f\"Calculated Focal Loss alpha (normalized): {focal_loss_alpha.cpu().numpy()}\")\n",
    "        else:\n",
    "            print(\"Warning: All class counts are zero. Cannot calculate Focal Loss alpha.\")\n",
    "    else:\n",
    "        print(f\"Warning: Number of found classes ({len(class_counts)}) in dataset does not match NUM_CLASSES ({NUM_CLASSES}). Focal Loss alpha set to None.\")\n",
    "\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Data directory not found. Please check TRAIN_DATA_DIR and VAL_DATA_DIR.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")\n",
    "    exit()\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True) # BATCH_SIZE는 학습과 동일하게 설정\n",
    "\n",
    "# --- 7. 손실 함수, 옵티마이저, 스케줄러 정의 ---\n",
    "if focal_loss_alpha is not None:\n",
    "    criterion = FocalLoss(alpha=focal_loss_alpha, gamma=2.0).to(DEVICE)\n",
    "    print(\"Using Focal Loss with calculated alpha.\")\n",
    "else:\n",
    "    criterion = FocalLoss(gamma=2.0).to(DEVICE) # 또는 nn.CrossEntropyLoss().to(DEVICE)\n",
    "    print(\"Using Focal Loss without alpha (or CrossEntropyLoss if preferred).\")\n",
    "\n",
    "optimizer = optim.AdamW(model_train.parameters(), lr=BASE_LR, weight_decay=WEIGHT_DECAY)\n",
    "lr_scheduler = None\n",
    "if EPOCHS > WARMUP_EPOCHS:\n",
    "    lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS - WARMUP_EPOCHS, eta_min=BASE_LR * 0.01)\n",
    "else: # WARMUP_EPOCHS >= EPOCHS 인 경우 스케줄러 없음\n",
    "    print(\"Warning: WARMUP_EPOCHS >= EPOCHS. No LR scheduler will be used after warmup.\")\n",
    "\n",
    "\n",
    "# --- 8. Custom Dataset for Test Images (추가된 부분) ---\n",
    "class TestImageDataset(Dataset):\n",
    "    def __init__(self, csv_path, img_dir_root, transform=None):\n",
    "        self.data_frame = pd.read_csv(csv_path)\n",
    "        self.img_dir_root = img_dir_root\n",
    "        self.transform = transform\n",
    "        if 'img_path' not in self.data_frame.columns:\n",
    "            print(f\"Warning: 'img_path' column not found in {csv_path}. Assuming first column ('{self.data_frame.columns[0]}') contains image paths.\")\n",
    "            self.img_path_column = self.data_frame.columns[0]\n",
    "        else:\n",
    "            self.img_path_column = 'img_path'\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        relative_img_path = self.data_frame.loc[idx, self.img_path_column]\n",
    "        full_img_path = os.path.join(self.img_dir_root, relative_img_path)\n",
    "        try:\n",
    "            image = Image.open(full_img_path).convert('RGB')\n",
    "        except FileNotFoundError:\n",
    "            print(f\"ERROR: Image not found at {full_img_path}. Check IMAGE_BASE_DIR and CSV paths.\")\n",
    "            # To avoid crashing the whole batch, return a dummy tensor and None path\n",
    "            # This requires handling in the prediction loop if it occurs.\n",
    "            # For now, we let it raise during dataset iteration if critical.\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: Could not open image {full_img_path}: {e}\")\n",
    "            raise\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, relative_img_path\n",
    "\n",
    "\n",
    "# --- 9. Inference Function (추가된 부분) ---\n",
    "def run_inference(\n",
    "    model_path: str,\n",
    "    model_architecture_name: str, # e.g., 'maxvit_xlarge_tf_384.in21k_ft_in1k'\n",
    "    num_classes_inf: int,\n",
    "    img_size_inf: int,\n",
    "    device_inf: torch.device,\n",
    "    test_csv_path_inf: str,\n",
    "    image_base_dir_inf: str,\n",
    "    output_csv_path_inf: str,\n",
    "    class_names_inf: list,\n",
    "    batch_size_inf: int = 16, # Can be different from training batch size\n",
    "    num_workers_inf: int = 4\n",
    "):\n",
    "    print(f\"\\n--- Starting Inference ---\")\n",
    "    print(f\"Loading model for inference: {model_architecture_name} from {model_path}\")\n",
    "\n",
    "    # Load the model structure\n",
    "    model_inf = timm.create_model(model_architecture_name, pretrained=False, num_classes=num_classes_inf, img_size=img_size_inf)\n",
    "    try:\n",
    "        model_inf.load_state_dict(torch.load(model_path, map_location=device_inf))\n",
    "        print(f\"Successfully loaded model weights from: {model_path}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: Model file not found at {model_path}. Cannot run inference.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Could not load model weights for inference: {e}\")\n",
    "        return\n",
    "\n",
    "    model_inf = model_inf.to(device_inf)\n",
    "    model_inf.eval()\n",
    "\n",
    "    # Re-create the inference transform (should be same as val_transform)\n",
    "    # We use the loaded model_inf to resolve config for robustness\n",
    "    try:\n",
    "        config_inf = timm.data.resolve_data_config({}, model=model_inf)\n",
    "        current_inference_transform = transforms.Compose([\n",
    "            transforms.Resize(img_size_inf, interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "            transforms.CenterCrop(img_size_inf),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=config_inf['mean'], std=config_inf['std'])\n",
    "        ])\n",
    "        print(\"Using timm's default transform for inference based on loaded model.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to get timm config for inference model ({e}), defining manually (ImageNet defaults).\")\n",
    "        current_inference_transform = transforms.Compose([\n",
    "            transforms.Resize(img_size_inf, interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "            transforms.CenterCrop(img_size_inf),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    print(f\"Inference Transform: {current_inference_transform}\")\n",
    "\n",
    "    # DataLoader for Test Set\n",
    "    print(f\"Loading test data from: {test_csv_path_inf} with image base: {image_base_dir_inf}\")\n",
    "    try:\n",
    "        test_dataset_inf = TestImageDataset(csv_path=test_csv_path_inf,\n",
    "                                        img_dir_root=image_base_dir_inf,\n",
    "                                        transform=current_inference_transform)\n",
    "        test_loader_inf = DataLoader(test_dataset_inf,\n",
    "                                 batch_size=batch_size_inf,\n",
    "                                 shuffle=False,\n",
    "                                 num_workers=num_workers_inf,\n",
    "                                 pin_memory=True)\n",
    "        print(f\"Found {len(test_dataset_inf)} images for testing.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Test CSV file not found at {test_csv_path_inf}. Cannot run inference.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating test dataset/loader for inference: {e}\")\n",
    "        return\n",
    "\n",
    "    # Prediction\n",
    "    all_preds_indices = []\n",
    "    all_filenames = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, filenames_batch in tqdm(test_loader_inf, desc=\"Predicting\"):\n",
    "            inputs = inputs.to(device_inf)\n",
    "            outputs = model_inf(inputs)\n",
    "            _, predicted_indices = torch.max(outputs, 1)\n",
    "            all_preds_indices.extend(predicted_indices.cpu().numpy())\n",
    "            all_filenames.extend(list(filenames_batch))\n",
    "\n",
    "    predicted_class_names_inf = [class_names_inf[idx] for idx in all_preds_indices]\n",
    "\n",
    "    # Create submission DataFrame\n",
    "    original_test_df = pd.read_csv(test_csv_path_inf)\n",
    "    csv_img_path_col = test_dataset_inf.img_path_column\n",
    "\n",
    "    prediction_map = {\n",
    "        os.path.normpath(p): label\n",
    "        for p, label in zip(all_filenames, predicted_class_names_inf)\n",
    "    }\n",
    "    mapped_labels = original_test_df[csv_img_path_col].apply(\n",
    "        lambda x: prediction_map.get(os.path.normpath(x))\n",
    "    )\n",
    "\n",
    "    submission_df_final = pd.DataFrame()\n",
    "    submission_df_final['ID'] = original_test_df[csv_img_path_col].apply(\n",
    "        lambda x: os.path.splitext(os.path.basename(x))[0]\n",
    "    )\n",
    "    submission_df_final['rock_type'] = mapped_labels\n",
    "\n",
    "    if submission_df_final['rock_type'].isnull().any():\n",
    "        num_null = submission_df_final['rock_type'].isnull().sum()\n",
    "        print(f\"Warning: {num_null} images in the CSV did not get a prediction. Check for mismatches or missing images.\")\n",
    "        # Example: Fill with a default class if needed (e.g., the most frequent one or 'Etc')\n",
    "        # default_class_for_nan = class_names_inf[0] # Or any other logic\n",
    "        # submission_df_final['rock_type'].fillna(default_class_for_nan, inplace=True)\n",
    "        # print(f\"Filled {num_null} NaNs with '{default_class_for_nan}'.\")\n",
    "\n",
    "\n",
    "    os.makedirs(os.path.dirname(output_csv_path_inf), exist_ok=True)\n",
    "    submission_df_final.to_csv(output_csv_path_inf, index=False)\n",
    "    print(f\"Inference complete. Predictions saved to: {output_csv_path_inf}\")\n",
    "    print(f\"Sample predictions:\\n{submission_df_final.head()}\")\n",
    "    print(f\"--- Inference Finished ---\")\n",
    "\n",
    "\n",
    "# --- 10. 학습 및 검증 실행 (그리고 추론) ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"\\nStarting training process with {MODEL_NAME}...\")\n",
    "    print(f\"Batch Size: {BATCH_SIZE}. If OOM errors, reduce it.\")\n",
    "    \n",
    "    history = train_and_validate_best_f1(\n",
    "        model=model_train, # Use the globally defined model for training\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        epochs=EPOCHS,\n",
    "        device=DEVICE,\n",
    "        num_classes=NUM_CLASSES,\n",
    "        save_dir=SAVE_DIR,\n",
    "        model_name_base=MODEL_ARTIFACT_NAME_BASE,\n",
    "        top_k=5,\n",
    "        gradient_clipping=GRADIENT_CLIPPING,\n",
    "        lr_scheduler=lr_scheduler,\n",
    "        warmup_epochs=WARMUP_EPOCHS,\n",
    "        base_lr=BASE_LR\n",
    "    )\n",
    "\n",
    "    print(\"\\nTraining finished.\")\n",
    "    if history['val_macro_f1_scores']:\n",
    "        best_f1_overall = max(history['val_macro_f1_scores'])\n",
    "        best_epoch_overall = history['val_macro_f1_scores'].index(best_f1_overall) + 1\n",
    "        print(f\"  Overall Best Validation Macro F1 in history: {best_f1_overall:.4f}\")\n",
    "        print(f\"  Achieved at Epoch: {best_epoch_overall}\")\n",
    "    else:\n",
    "        print(\"No validation scores recorded in history.\")\n",
    "\n",
    "    # --- 자동 추론 실행 ---\n",
    "    if history.get('best_model_path') and os.path.exists(history['best_model_path']):\n",
    "        best_model_for_inference = history['best_model_path']\n",
    "        # Extract F1 score from filename for output CSV name\n",
    "        try:\n",
    "            f1_score_from_filename = float(os.path.basename(best_model_for_inference).split('_f1_')[1].replace('.pth',''))\n",
    "            output_csv_filename = f\"submission_{MODEL_ARTIFACT_NAME_BASE}_f1_{f1_score_from_filename:.4f}.csv\"\n",
    "        except: # Fallback if filename parsing fails\n",
    "            output_csv_filename = f\"submission_{MODEL_ARTIFACT_NAME_BASE}_best.csv\"\n",
    "        \n",
    "        final_output_csv_path = os.path.join(SAVE_DIR, output_csv_filename) # Save submission in the model save directory\n",
    "\n",
    "        if not CLASS_NAMES:\n",
    "             print(\"ERROR: CLASS_NAMES are not set. Cannot run inference. Please define them correctly.\")\n",
    "        else:\n",
    "            run_inference(\n",
    "                model_path=best_model_for_inference,\n",
    "                model_architecture_name=MODEL_NAME, # Use the same architecture as training\n",
    "                num_classes_inf=NUM_CLASSES,\n",
    "                img_size_inf=IMG_SIZE,\n",
    "                device_inf=DEVICE,\n",
    "                test_csv_path_inf=TEST_CSV_PATH,\n",
    "                image_base_dir_inf=IMAGE_BASE_DIR,\n",
    "                output_csv_path_inf=final_output_csv_path,\n",
    "                class_names_inf=CLASS_NAMES,\n",
    "                batch_size_inf=BATCH_SIZE * 2, # Inference can often use larger batch size\n",
    "                num_workers_inf=NUM_WORKERS\n",
    "            )\n",
    "    else:\n",
    "        print(\"\\nNo best model path found or model file does not exist. Skipping inference.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb321a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model architecture: maxvit_xlarge_tf_384.in21k_ft_in1k\n",
      "Loading trained weights from: ./saved_models_maxvit_xlarge_384/maxvit_xlarge_tf_384_epoch15_f1_0.9192.pth\n",
      "Successfully loaded model weights onto cuda.\n",
      "Model ready for inference.\n",
      "Defining inference transform...\n",
      "Using timm's default transform based on loaded model config: Mean=(0.5, 0.5, 0.5), Std=(0.5, 0.5, 0.5)\n",
      "Inference Transform: Compose(\n",
      "    Resize(size=(384, 384), interpolation=bicubic, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(384, 384))\n",
      "    MaybeToTensor()\n",
      "    Normalize(mean=tensor([0.5000, 0.5000, 0.5000]), std=tensor([0.5000, 0.5000, 0.5000]))\n",
      ")\n",
      "Loading test data from CSV: /home/metaai2/workspace/limseunghwan/open/test.csv\n",
      "Image base directory: /home/metaai2/workspace/limseunghwan/open\n",
      "Using column 'img_path' from CSV for image paths.\n",
      "Successfully created DataLoader with 95006 images for testing.\n",
      "\n",
      "Starting inference process...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:   8%|▊         | 1841/23752 [03:07<37:15,  9.80it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 187\u001b[39m\n\u001b[32m    184\u001b[39m output_csv_filename = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33msubmission_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_filename_base\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.csv\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    185\u001b[39m final_output_csv_path = os.path.join(OUTPUT_CSV_DIR, output_csv_filename)\n\u001b[32m--> \u001b[39m\u001b[32m187\u001b[39m original_paths_from_loader, string_predictions = \u001b[43mpredict_on_test_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    190\u001b[39m \u001b[43m    \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m    \u001b[49m\u001b[43mCLASS_NAMES\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[38;5;66;03m# Create a DataFrame for submission\u001b[39;00m\n\u001b[32m    195\u001b[39m \u001b[38;5;66;03m# Ensure the order of predictions matches the order of images in the original test.csv\u001b[39;00m\n\u001b[32m    196\u001b[39m \n\u001b[32m    197\u001b[39m \u001b[38;5;66;03m# Load the original test CSV to get the correct order and 'ID' column format if needed\u001b[39;00m\n\u001b[32m    198\u001b[39m original_test_df = pd.read_csv(TEST_CSV_PATH)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 169\u001b[39m, in \u001b[36mpredict_on_test_data\u001b[39m\u001b[34m(model_to_predict, loader, device, class_names_map)\u001b[39m\n\u001b[32m    166\u001b[39m         outputs = model_to_predict(images_batch)\n\u001b[32m    167\u001b[39m         _, predicted_indices_batch = torch.max(outputs, \u001b[32m1\u001b[39m) \u001b[38;5;66;03m# Get the index of the max log-probability\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m169\u001b[39m         all_predicted_indices.extend(\u001b[43mpredicted_indices_batch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.numpy())\n\u001b[32m    170\u001b[39m         all_original_filenames.extend(\u001b[38;5;28mlist\u001b[39m(filenames_batch)) \u001b[38;5;66;03m# filenames_batch is a tuple of strings\u001b[39;00m\n\u001b[32m    172\u001b[39m \u001b[38;5;66;03m# Convert predicted indices to actual class names\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# MaxViT_inference.py\n",
    "import torch\n",
    "import torch.nn as nn # Not strictly needed for inference if model is loaded, but good practice\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "import timm\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np # For potential use, e.g. if needing to map class indices to names manually\n",
    "\n",
    "# --- 1. Configuration (Adjust these paths and parameters) ---\n",
    "# Model and Data Parameters (should match how the loaded model was trained)\n",
    "MODEL_ARCHITECTURE_NAME = 'maxvit_xlarge_tf_384.in21k_ft_in1k' # Architecture of your saved model\n",
    "NUM_CLASSES = 7        # Number of classes your model was trained on\n",
    "IMG_SIZE = 384         # Image size your model was trained with\n",
    "\n",
    "# --- Paths ---\n",
    "# TODO: IMPORTANT! Update this to the path of YOUR TRAINED MaxViT model .pth file\n",
    "SAVED_MODEL_PATH = './saved_models_maxvit_xlarge_384/maxvit_xlarge_tf_384_epoch15_f1_0.9192.pth' # EXAMPLE PATH!\n",
    "TEST_CSV_PATH = r'/home/metaai2/workspace/limseunghwan/open/test.csv'    # Path to your test.csv\n",
    "IMAGE_BASE_DIR = r'/home/metaai2/workspace/limseunghwan/open'           # Base directory for images in test.csv\n",
    "OUTPUT_CSV_DIR = './submissions_maxvit/maxvit_xlarge_tf_384_epoch15_f1_0.9192' # Directory to save the output CSV\n",
    "# OUTPUT_CSV_PATH will be generated dynamically based on model name.\n",
    "\n",
    "# --- Inference Parameters ---\n",
    "BATCH_SIZE_INFERENCE = 4 # Adjust based on your GPU memory (MaxViT XLarge is demanding)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_WORKERS_INFERENCE = os.cpu_count() // 2 if os.cpu_count() else 4\n",
    "\n",
    "# --- Class Names (CRITICAL!) ---\n",
    "# TODO: ***매우 중요*** 당신의 데이터셋에 맞는 실제 클래스 이름을 순서대로 정의하세요.\n",
    "#       이 순서는 모델이 학습될 때 클래스에 할당된 인덱스와 일치해야 합니다.\n",
    "#       (예: train_dataset.classes 에서 가져온 순서)\n",
    "CLASS_NAMES = ['Andesite', 'Basalt', 'Etc', 'Gneiss', 'Granite', 'Mud_Sandstone', 'Weathered_Rock'] # 예시 값, 반드시 수정!\n",
    "\n",
    "if len(CLASS_NAMES) != NUM_CLASSES:\n",
    "    raise ValueError(f\"Number of CLASS_NAMES ({len(CLASS_NAMES)}) does not match NUM_CLASSES ({NUM_CLASSES}). Please check your configuration.\")\n",
    "\n",
    "# --- 2. Model Loading ---\n",
    "print(f\"Loading model architecture: {MODEL_ARCHITECTURE_NAME}\")\n",
    "# Create the model structure (pretrained=False because we load our own weights)\n",
    "model = timm.create_model(\n",
    "    MODEL_ARCHITECTURE_NAME,\n",
    "    pretrained=False,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    img_size=IMG_SIZE  # Ensure img_size is passed if model supports/requires it at creation\n",
    ")\n",
    "\n",
    "print(f\"Loading trained weights from: {SAVED_MODEL_PATH}\")\n",
    "if not os.path.exists(SAVED_MODEL_PATH):\n",
    "    print(f\"ERROR: Model weights file not found at {SAVED_MODEL_PATH}. Please check the path.\")\n",
    "    exit()\n",
    "\n",
    "try:\n",
    "    # Load the state dictionary\n",
    "    model.load_state_dict(torch.load(SAVED_MODEL_PATH, map_location=DEVICE))\n",
    "    print(f\"Successfully loaded model weights onto {DEVICE}.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Could not load model weights: {e}\")\n",
    "    exit()\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "model.eval() # Set the model to evaluation mode\n",
    "print(\"Model ready for inference.\")\n",
    "\n",
    "# --- 3. Data Transformations for Inference ---\n",
    "# Re-create the validation/inference transform used during training.\n",
    "# It's best if this matches exactly what was used for the validation set.\n",
    "print(\"Defining inference transform...\")\n",
    "try:\n",
    "    # Attempt to use timm's recommended settings for the loaded model\n",
    "    # Pass the loaded model instance to resolve_data_config\n",
    "    config = timm.data.resolve_data_config({}, model=model)\n",
    "    # Override input_size to be sure it matches your training\n",
    "    config['input_size'] = (3, IMG_SIZE, IMG_SIZE) # (C, H, W)\n",
    "    \n",
    "    inference_transform = timm.data.create_transform(**config, is_training=False)\n",
    "    print(f\"Using timm's default transform based on loaded model config: Mean={config['mean']}, Std={config['std']}\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to get timm config for inference model ({e}). Defining transform manually using ImageNet defaults.\")\n",
    "    inference_transform = transforms.Compose([\n",
    "        transforms.Resize(IMG_SIZE, interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "        transforms.CenterCrop(IMG_SIZE),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # ImageNet defaults\n",
    "    ])\n",
    "print(f\"Inference Transform: {inference_transform}\")\n",
    "\n",
    "\n",
    "# --- 4. Custom Dataset for Test Images ---\n",
    "class TestImageDataset(Dataset):\n",
    "    def __init__(self, csv_path, img_dir_root, transform=None):\n",
    "        self.data_frame = pd.read_csv(csv_path)\n",
    "        self.img_dir_root = img_dir_root\n",
    "        self.transform = transform\n",
    "        # Determine the image path column name\n",
    "        if 'img_path' not in self.data_frame.columns:\n",
    "            print(f\"Warning: 'img_path' column not found in {csv_path}. Assuming first column ('{self.data_frame.columns[0]}') contains image paths.\")\n",
    "            self.img_path_column = self.data_frame.columns[0]\n",
    "        else:\n",
    "            self.img_path_column = 'img_path'\n",
    "        print(f\"Using column '{self.img_path_column}' from CSV for image paths.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        relative_img_path = self.data_frame.loc[idx, self.img_path_column]\n",
    "        full_img_path = os.path.join(self.img_dir_root, relative_img_path)\n",
    "\n",
    "        try:\n",
    "            image = Image.open(full_img_path).convert('RGB')\n",
    "        except FileNotFoundError:\n",
    "            print(f\"ERROR: Image not found at {full_img_path}. Check IMAGE_BASE_DIR and CSV paths.\")\n",
    "            # To prevent crashing the DataLoader, one might return a placeholder or skip.\n",
    "            # For now, raising an error is fine to highlight the issue.\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: Could not open image {full_img_path}: {e}\")\n",
    "            raise\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, relative_img_path # Return image and its original path for mapping\n",
    "\n",
    "# --- 5. DataLoader for Test Set ---\n",
    "print(f\"Loading test data from CSV: {TEST_CSV_PATH}\")\n",
    "print(f\"Image base directory: {IMAGE_BASE_DIR}\")\n",
    "try:\n",
    "    test_dataset = TestImageDataset(csv_path=TEST_CSV_PATH,\n",
    "                                    img_dir_root=IMAGE_BASE_DIR,\n",
    "                                    transform=inference_transform)\n",
    "    if len(test_dataset) == 0:\n",
    "        print(f\"ERROR: No images found or loaded from {TEST_CSV_PATH}. Please check the CSV and image paths.\")\n",
    "        exit()\n",
    "        \n",
    "    test_loader = DataLoader(test_dataset,\n",
    "                             batch_size=BATCH_SIZE_INFERENCE,\n",
    "                             shuffle=False, # No need to shuffle for inference\n",
    "                             num_workers=NUM_WORKERS_INFERENCE,\n",
    "                             pin_memory=True)\n",
    "    print(f\"Successfully created DataLoader with {len(test_dataset)} images for testing.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Test CSV file not found at {TEST_CSV_PATH}. Please check the path.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Could not create test dataset or DataLoader: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- 6. Prediction Function ---\n",
    "def predict_on_test_data(model_to_predict, loader, device, class_names_map):\n",
    "    model_to_predict.eval() # Ensure model is in eval mode\n",
    "    all_predicted_indices = []\n",
    "    all_original_filenames = [] # To store the original filenames/paths from CSV\n",
    "\n",
    "    with torch.no_grad(): # Disable gradient calculations for inference\n",
    "        for images_batch, filenames_batch in tqdm(loader, desc=\"Predicting\"):\n",
    "            images_batch = images_batch.to(device)\n",
    "            \n",
    "            outputs = model_to_predict(images_batch)\n",
    "            _, predicted_indices_batch = torch.max(outputs, 1) # Get the index of the max log-probability\n",
    "\n",
    "            all_predicted_indices.extend(predicted_indices_batch.cpu().numpy())\n",
    "            all_original_filenames.extend(list(filenames_batch)) # filenames_batch is a tuple of strings\n",
    "\n",
    "    # Convert predicted indices to actual class names\n",
    "    predicted_class_names_list = [class_names_map[idx] for idx in all_predicted_indices]\n",
    "    \n",
    "    return all_original_filenames, predicted_class_names_list\n",
    "\n",
    "# --- 7. Run Inference and Save Results ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\nStarting inference process...\")\n",
    "\n",
    "    # Generate output CSV path\n",
    "    os.makedirs(OUTPUT_CSV_DIR, exist_ok=True)\n",
    "    model_filename_base = os.path.splitext(os.path.basename(SAVED_MODEL_PATH))[0]\n",
    "    output_csv_filename = f\"submission_{model_filename_base}.csv\"\n",
    "    final_output_csv_path = os.path.join(OUTPUT_CSV_DIR, output_csv_filename)\n",
    "\n",
    "    original_paths_from_loader, string_predictions = predict_on_test_data(\n",
    "        model,\n",
    "        test_loader,\n",
    "        DEVICE,\n",
    "        CLASS_NAMES\n",
    "    )\n",
    "\n",
    "    # Create a DataFrame for submission\n",
    "    # Ensure the order of predictions matches the order of images in the original test.csv\n",
    "    \n",
    "    # Load the original test CSV to get the correct order and 'ID' column format if needed\n",
    "    original_test_df = pd.read_csv(TEST_CSV_PATH)\n",
    "    \n",
    "    # Create a mapping from the image paths returned by DataLoader to their predictions\n",
    "    # Use os.path.normpath to handle potential path separator differences (e.g. / vs \\)\n",
    "    prediction_map = {\n",
    "        os.path.normpath(path): pred_label \n",
    "        for path, pred_label in zip(original_paths_from_loader, string_predictions)\n",
    "    }\n",
    "\n",
    "    # Map predictions back to the original CSV's image paths\n",
    "    # This ensures that even if DataLoader reorders (it shouldn't with shuffle=False),\n",
    "    # or if there are missing images handled gracefully, the mapping is correct.\n",
    "    csv_img_path_col = test_dataset.img_path_column # Get the column name used for image paths\n",
    "    \n",
    "    mapped_predictions = original_test_df[csv_img_path_col].apply(\n",
    "        lambda x: prediction_map.get(os.path.normpath(x))\n",
    "    )\n",
    "\n",
    "    # Create the submission DataFrame\n",
    "    submission_df = pd.DataFrame()\n",
    "    # The 'ID' column in submission usually requires the filename without extension\n",
    "    submission_df['ID'] = original_test_df[csv_img_path_col].apply(\n",
    "        lambda x: os.path.splitext(os.path.basename(x))[0]\n",
    "    )\n",
    "    submission_df['rock_type'] = mapped_predictions # Use the mapped predictions\n",
    "\n",
    "    # Check for any images that didn't get a prediction (should not happen if all images load)\n",
    "    if submission_df['rock_type'].isnull().any():\n",
    "        num_null = submission_df['rock_type'].isnull().sum()\n",
    "        print(f\"WARNING: {num_null} images from the CSV did not receive a prediction. \"\n",
    "              \"This might be due to missing image files or errors during loading. \"\n",
    "              \"Consider filling NaNs if appropriate for the submission.\")\n",
    "        # Example: Fill with a default class if needed (e.g., the most frequent one or 'Etc')\n",
    "        # default_class_for_nan = CLASS_NAMES[0] # Or any other logic\n",
    "        # submission_df['rock_type'].fillna(default_class_for_nan, inplace=True)\n",
    "        # print(f\"Filled {num_null} NaNs with '{default_class_for_nan}'.\")\n",
    "\n",
    "\n",
    "    # Save the submission file\n",
    "    submission_df.to_csv(final_output_csv_path, index=False)\n",
    "    print(f\"\\nInference complete. Predictions saved to: {final_output_csv_path}\")\n",
    "    print(f\"Sample of the submission file:\\n{submission_df.head()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5f7d77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env312_cuda124_torch260",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
