{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae723cdc",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed1f2776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linux\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "print(platform.system()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb13b32c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Version: 12.4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# PyTorch의 CUDA 버전 확인\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"CUDA는 사용할 수 없습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a44f1ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 파일 개수 (서브폴더 포함): 33452\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "folder_path = './open/train/Weathered_Rock/'\n",
    "total_file_count = 0\n",
    "\n",
    "for root, dirs, files in os.walk(folder_path):\n",
    "    total_file_count += len(files)\n",
    "\n",
    "print(f\"전체 파일 개수 (서브폴더 포함): {total_file_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aff39751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['convnext_atto.d2_in1k', 'convnext_atto_ols.a2_in1k', 'convnext_base.clip_laion2b', 'convnext_base.clip_laion2b_augreg', 'convnext_base.clip_laion2b_augreg_ft_in1k', 'convnext_base.clip_laion2b_augreg_ft_in12k', 'convnext_base.clip_laion2b_augreg_ft_in12k_in1k', 'convnext_base.clip_laion2b_augreg_ft_in12k_in1k_384', 'convnext_base.clip_laiona', 'convnext_base.clip_laiona_320', 'convnext_base.clip_laiona_augreg_320', 'convnext_base.clip_laiona_augreg_ft_in1k_384', 'convnext_base.fb_in1k', 'convnext_base.fb_in22k', 'convnext_base.fb_in22k_ft_in1k', 'convnext_base.fb_in22k_ft_in1k_384', 'convnext_femto.d1_in1k', 'convnext_femto_ols.d1_in1k', 'convnext_large.fb_in1k', 'convnext_large.fb_in22k', 'convnext_large.fb_in22k_ft_in1k', 'convnext_large.fb_in22k_ft_in1k_384', 'convnext_large_mlp.clip_laion2b_augreg', 'convnext_large_mlp.clip_laion2b_augreg_ft_in1k', 'convnext_large_mlp.clip_laion2b_augreg_ft_in1k_384', 'convnext_large_mlp.clip_laion2b_augreg_ft_in12k_384', 'convnext_large_mlp.clip_laion2b_ft_320', 'convnext_large_mlp.clip_laion2b_ft_soup_320', 'convnext_large_mlp.clip_laion2b_soup_ft_in12k_320', 'convnext_large_mlp.clip_laion2b_soup_ft_in12k_384', 'convnext_large_mlp.clip_laion2b_soup_ft_in12k_in1k_320', 'convnext_large_mlp.clip_laion2b_soup_ft_in12k_in1k_384', 'convnext_nano.d1h_in1k', 'convnext_nano.in12k', 'convnext_nano.in12k_ft_in1k', 'convnext_nano.r384_ad_in12k', 'convnext_nano.r384_in12k', 'convnext_nano.r384_in12k_ft_in1k', 'convnext_nano_ols.d1h_in1k', 'convnext_pico.d1_in1k', 'convnext_pico_ols.d1_in1k', 'convnext_small.fb_in1k', 'convnext_small.fb_in22k', 'convnext_small.fb_in22k_ft_in1k', 'convnext_small.fb_in22k_ft_in1k_384', 'convnext_small.in12k', 'convnext_small.in12k_ft_in1k', 'convnext_small.in12k_ft_in1k_384', 'convnext_tiny.fb_in1k', 'convnext_tiny.fb_in22k', 'convnext_tiny.fb_in22k_ft_in1k', 'convnext_tiny.fb_in22k_ft_in1k_384', 'convnext_tiny.in12k', 'convnext_tiny.in12k_ft_in1k', 'convnext_tiny.in12k_ft_in1k_384', 'convnext_tiny_hnf.a2h_in1k', 'convnext_xlarge.fb_in22k', 'convnext_xlarge.fb_in22k_ft_in1k', 'convnext_xlarge.fb_in22k_ft_in1k_384', 'convnext_xxlarge.clip_laion2b_rewind', 'convnext_xxlarge.clip_laion2b_soup', 'convnext_xxlarge.clip_laion2b_soup_ft_in1k', 'convnext_xxlarge.clip_laion2b_soup_ft_in12k', 'convnext_zepto_rms.ra4_e3600_r224_in1k', 'convnext_zepto_rms_ols.ra4_e3600_r224_in1k', 'convnextv2_atto.fcmae', 'convnextv2_atto.fcmae_ft_in1k', 'convnextv2_base.fcmae', 'convnextv2_base.fcmae_ft_in1k', 'convnextv2_base.fcmae_ft_in22k_in1k', 'convnextv2_base.fcmae_ft_in22k_in1k_384', 'convnextv2_femto.fcmae', 'convnextv2_femto.fcmae_ft_in1k', 'convnextv2_huge.fcmae', 'convnextv2_huge.fcmae_ft_in1k', 'convnextv2_huge.fcmae_ft_in22k_in1k_384', 'convnextv2_huge.fcmae_ft_in22k_in1k_512', 'convnextv2_large.fcmae', 'convnextv2_large.fcmae_ft_in1k', 'convnextv2_large.fcmae_ft_in22k_in1k', 'convnextv2_large.fcmae_ft_in22k_in1k_384', 'convnextv2_nano.fcmae', 'convnextv2_nano.fcmae_ft_in1k', 'convnextv2_nano.fcmae_ft_in22k_in1k', 'convnextv2_nano.fcmae_ft_in22k_in1k_384', 'convnextv2_pico.fcmae', 'convnextv2_pico.fcmae_ft_in1k', 'convnextv2_tiny.fcmae', 'convnextv2_tiny.fcmae_ft_in1k', 'convnextv2_tiny.fcmae_ft_in22k_in1k', 'convnextv2_tiny.fcmae_ft_in22k_in1k_384', 'test_convnext2.r160_in1k', 'test_convnext3.r160_in1k', 'test_convnext.r160_in1k']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/metaai2/miniconda3/envs/env312_cuda124_torch260/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "import torch\n",
    "\n",
    "# 별도로 해당 모델에 맞는 입력 전처리 방법을 확인하고 적용해야 합니다.\n",
    "# 위 코드의 resolve_data_config와 create_transform 부분이 바로 이 역할을 합니다. \n",
    "# timm을 사용할 때는 이 전처리 부분을 놓치지 않는 것이 매우 중요합니다.\n",
    "available_swin_models = timm.list_models('*convnext*', pretrained=True)\n",
    "print(available_swin_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05352743",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"convnext_large.fb_in22k_ft_in1k_384\"\n",
    "transfer_model_convnext = timm.create_model(model_name, pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4047ad72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "print(next(transfer_model_convnext.parameters()).device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc28bf17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNeXt(\n",
       "  (stem): Sequential(\n",
       "    (0): Conv2d(3, 192, kernel_size=(4, 4), stride=(4, 4))\n",
       "    (1): LayerNorm2d((192,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (stages): Sequential(\n",
       "    (0): ConvNeXtStage(\n",
       "      (downsample): Identity()\n",
       "      (blocks): Sequential(\n",
       "        (0): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "          (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (1): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "          (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (2): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "          (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): ConvNeXtStage(\n",
       "      (downsample): Sequential(\n",
       "        (0): LayerNorm2d((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))\n",
       "      )\n",
       "      (blocks): Sequential(\n",
       "        (0): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (1): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (2): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): ConvNeXtStage(\n",
       "      (downsample): Sequential(\n",
       "        (0): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))\n",
       "      )\n",
       "      (blocks): Sequential(\n",
       "        (0): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (1): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (2): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (3): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (4): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (5): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (6): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (7): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (8): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (9): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (10): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (11): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (12): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (13): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (14): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (15): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (16): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (17): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (18): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (19): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (20): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (21): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (22): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (23): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (24): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (25): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (26): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): ConvNeXtStage(\n",
       "      (downsample): Sequential(\n",
       "        (0): LayerNorm2d((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (1): Conv2d(768, 1536, kernel_size=(2, 2), stride=(2, 2))\n",
       "      )\n",
       "      (blocks): Sequential(\n",
       "        (0): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(1536, 1536, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1536)\n",
       "          (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=6144, out_features=1536, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (1): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(1536, 1536, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1536)\n",
       "          (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=6144, out_features=1536, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (2): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(1536, 1536, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1536)\n",
       "          (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=6144, out_features=1536, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm_pre): Identity()\n",
       "  (head): NormMlpClassifierHead(\n",
       "    (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Identity())\n",
       "    (norm): LayerNorm2d((1536,), eps=1e-06, elementwise_affine=True)\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "    (pre_logits): Identity()\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (fc): Linear(in_features=1536, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transfer_model_convnext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e74586a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 7 # 암석 종류 7가지\n",
    "in_features = transfer_model_convnext.head.fc.in_features # transfer_model_swin.head로 transfer_model_swin.head 인지 .fc를 붙이는 지 차이가 있다.\n",
    "transfer_model_convnext.head.fc = torch.nn.Linear(in_features, num_classes)  # fc 레이어 교체"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7ca7312",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NormMlpClassifierHead(\n",
       "  (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Identity())\n",
       "  (norm): LayerNorm2d((1536,), eps=1e-06, elementwise_affine=True)\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (pre_logits): Identity()\n",
       "  (drop): Dropout(p=0.0, inplace=False)\n",
       "  (fc): Linear(in_features=1536, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transfer_model_convnext.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd927b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: convnext_large.fb_in22k_ft_in1k_384\n",
      "Model head replaced for 7 classes.\n",
      "Using timm's default validation transform.\n",
      "Loading datasets from: /home/metaai2/workspace/limseunghwan/open/train and /home/metaai2/workspace/limseunghwan/open/val\n",
      "Found 342015 training images and 38005 validation images.\n",
      "Classes: ['Andesite', 'Basalt', 'Etc', 'Gneiss', 'Granite', 'Mud_Sandstone', 'Weathered_Rock']\n",
      "Calculated Focal Loss alpha (normalized): [0.36379087 0.59434706 1.         0.21558282 0.17148152 0.17810482\n",
      " 0.4287038 ]\n",
      "\n",
      "Starting training process...\n",
      "학습 시작: 총 25 에폭, Device: cuda\n",
      "Top-5 모델 저장 디렉토리: ./saved_models\n",
      "평가 기준: Validation Macro F1 Score\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                 \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 335\u001b[39m\n\u001b[32m    333\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mStarting training process...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    334\u001b[39m model_name_base = MODEL_NAME.split(\u001b[33m'\u001b[39m\u001b[33m/\u001b[39m\u001b[33m'\u001b[39m)[-\u001b[32m1\u001b[39m].split(\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m)[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33m/\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m MODEL_NAME \u001b[38;5;28;01melse\u001b[39;00m MODEL_NAME.split(\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m)[\u001b[32m0\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m335\u001b[39m history = \u001b[43mtrain_and_validate_best_f1\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    337\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    338\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    339\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    340\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    342\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    343\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mNUM_CLASSES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    344\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mSAVE_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;66;43;03m# 디렉토리 전달\u001b[39;49;00m\n\u001b[32m    345\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name_base\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_name_base\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# 모델 이름 기반 전달\u001b[39;49;00m\n\u001b[32m    346\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                       \u001b[49m\u001b[38;5;66;43;03m# 저장할 개수 지정 (예: 5)\u001b[39;49;00m\n\u001b[32m    347\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgradient_clipping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mGRADIENT_CLIPPING\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwarmup_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mWARMUP_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbase_lr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBASE_LR\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTraining finished.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    354\u001b[39m \u001b[38;5;66;03m# 최종 결과 요약 출력 (함수 내에서도 출력됨)\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 121\u001b[39m, in \u001b[36mtrain_and_validate_best_f1\u001b[39m\u001b[34m(model, train_loader, val_loader, optimizer, criterion, epochs, device, num_classes, save_dir, model_name_base, top_k, gradient_clipping, lr_scheduler, warmup_epochs, base_lr)\u001b[39m\n\u001b[32m    119\u001b[39m         torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clipping)\n\u001b[32m    120\u001b[39m     optimizer.step()\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m     running_train_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    122\u001b[39m     train_pbar.set_postfix(loss=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss.item()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    123\u001b[39m epoch_train_loss = running_train_loss / \u001b[38;5;28mlen\u001b[39m(train_loader)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# convnext.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms # torchvision.transforms 사용\n",
    "import torch.nn.functional as F # FocalLoss 등에서 필요\n",
    "import timm\n",
    "import torchmetrics\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import os\n",
    "import numpy as np # 필요시 사용\n",
    "\n",
    "# --- 1. Focal Loss 클래스 정의 (이전 코드에서 복사) ---\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2., reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            # alpha 처리 로직 (이전 코드 참조)\n",
    "            if isinstance(self.alpha, (float, int)):\n",
    "                 alpha = torch.tensor([self.alpha] * inputs.shape[1], device=inputs.device)\n",
    "            elif isinstance(self.alpha, list):\n",
    "                 alpha = torch.tensor(self.alpha, device=inputs.device, dtype=torch.float32)\n",
    "            elif torch.is_tensor(self.alpha):\n",
    "                 alpha = self.alpha.to(device=inputs.device, dtype=torch.float32)\n",
    "            else:\n",
    "                 raise TypeError(\"alpha must be float, list or torch.Tensor\")\n",
    "\n",
    "            if alpha.shape[0] != inputs.shape[1]:\n",
    "                 raise ValueError(f\"alpha size {alpha.shape[0]} does not match C {inputs.shape[1]}\")\n",
    "\n",
    "            alpha_t = alpha.gather(0, targets)\n",
    "            focal_loss = alpha_t * focal_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else: # 'none'\n",
    "            return focal_loss\n",
    "\n",
    "# --- 2. 학습 및 검증 함수 정의 (이전 코드에서 복사) ---\n",
    "# def train_and_validate_best_f1(...):\n",
    "#     ... (이전 코드 전체 복사) ...\n",
    "# --- 여기에 train_and_validate_best_f1 함수 코드를 붙여넣으세요 ---\n",
    "def train_and_validate_best_f1(model: nn.Module,\n",
    "                               train_loader: DataLoader,\n",
    "                               val_loader: DataLoader,\n",
    "                               optimizer: optim.Optimizer, # torch.optim 임포트 사용\n",
    "                               criterion: nn.Module, # Loss function\n",
    "                               epochs: int,\n",
    "                               device: torch.device,\n",
    "                               num_classes: int,\n",
    "                               save_dir: str, # 디렉토리로 변경\n",
    "                               model_name_base: str, # 모델 파일 이름용\n",
    "                               top_k: int = 5, # 저장할 상위 모델 개수 (기본값 5)\n",
    "                               gradient_clipping: float = None,\n",
    "                               lr_scheduler = None,\n",
    "                               warmup_epochs: int = 0,\n",
    "                               base_lr: float = 1e-5\n",
    "                              ):\n",
    "    \n",
    "    history = {'train_losses': [], 'val_losses': [], 'val_macro_f1_scores': []}\n",
    "    max_val_f1 = 0.0  # 여전히 전체 최고 점수 추적 (조기 종료용)\n",
    "    best_epoch = -1 # 최고 점수 달성 에폭\n",
    "    top_k_checkpoints = [] # (f1_score, file_path) 튜플을 저장할 리스트\n",
    "\n",
    "    f1_metric = torchmetrics.F1Score(task='multiclass', num_classes=num_classes, average='macro').to(device)\n",
    "    patience = 5\n",
    "    epochs_no_improve = 0 # 조기 종료 카운터는 여전히 max_val_f1 기준\n",
    "\n",
    "    # save_dir 존재 확인 및 생성 (함수 호출 전에 해도 되지만 여기서도 확인)\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"학습 시작: 총 {epochs} 에폭, Device: {device}\")\n",
    "    print(f\"Top-{top_k} 모델 저장 디렉토리: {save_dir}\") # 경로 출력 수정\n",
    "    print(f\"평가 기준: Validation Macro F1 Score\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "        # --- Warmup 및 LR 스케줄링 ---\n",
    "        if epoch < warmup_epochs:\n",
    "            warmup_factor = (epoch + 1) / warmup_epochs\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = base_lr * warmup_factor\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "        elif lr_scheduler is not None:\n",
    "             if epoch == warmup_epochs:\n",
    "                 for param_group in optimizer.param_groups:\n",
    "                     param_group['lr'] = base_lr\n",
    "                 lr_scheduler.step()\n",
    "             else:\n",
    "                 lr_scheduler.step()\n",
    "             current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "        # --- 학습 단계 ---\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "        train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Train] LR: {current_lr:.1e}\", leave=False)\n",
    "        for images, labels in train_pbar:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            if gradient_clipping is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clipping)\n",
    "            optimizer.step()\n",
    "            running_train_loss += loss.item()\n",
    "            train_pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "        epoch_train_loss = running_train_loss / len(train_loader)\n",
    "        history['train_losses'].append(epoch_train_loss)\n",
    "\n",
    "        # --- 검증 단계 ---\n",
    "        model.eval()\n",
    "        running_val_loss = 0.0\n",
    "        f1_metric.reset()\n",
    "        val_pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} [Val] \", leave=False)\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_pbar:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                running_val_loss += loss.item()\n",
    "                f1_metric.update(outputs, labels)\n",
    "                val_pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "        epoch_val_loss = running_val_loss / len(val_loader)\n",
    "        history['val_losses'].append(epoch_val_loss)\n",
    "        epoch_val_f1 = f1_metric.compute().item()\n",
    "        history['val_macro_f1_scores'].append(epoch_val_f1)\n",
    "\n",
    "        epoch_end_time = time.time()\n",
    "        epoch_duration = epoch_end_time - epoch_start_time\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] ({epoch_duration:.2f}s) - \"\n",
    "              f\"Train Loss: {epoch_train_loss:.4f}, \"\n",
    "              f\"Val Loss: {epoch_val_loss:.4f}, \"\n",
    "              f\"Val Macro F1: {epoch_val_f1:.4f}\")\n",
    "\n",
    "        # --- 모델 저장 및 조기 종료 ---\n",
    "        is_top_k = len(top_k_checkpoints) < top_k or epoch_val_f1 > top_k_checkpoints[-1][0]\n",
    "\n",
    "        if is_top_k:\n",
    "            # 새 모델 저장 경로 생성 (에폭과 F1 점수 포함)\n",
    "            checkpoint_filename = f\"{model_name_base}_epoch{epoch+1}_f1_{epoch_val_f1:.4f}.pth\"\n",
    "            checkpoint_path = os.path.join(save_dir, checkpoint_filename)\n",
    "\n",
    "            try:\n",
    "                torch.save(model.state_dict(), checkpoint_path)\n",
    "                print(f\"  Checkpoint saved to {checkpoint_path}\")\n",
    "\n",
    "                # Top-K 리스트 업데이트\n",
    "                top_k_checkpoints.append((epoch_val_f1, checkpoint_path))\n",
    "                # F1 점수 기준 내림차순 정렬\n",
    "                top_k_checkpoints.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "                # 리스트 크기가 K개를 초과하면 가장 낮은 점수 모델 제거\n",
    "                if len(top_k_checkpoints) > top_k:\n",
    "                    score_to_remove, path_to_remove = top_k_checkpoints.pop() # 마지막 항목 제거\n",
    "                    print(f\"  Removing checkpoint {os.path.basename(path_to_remove)} (score: {score_to_remove:.4f}) as it's no longer in top-{top_k}\")\n",
    "                    if os.path.exists(path_to_remove):\n",
    "                        try:\n",
    "                            os.remove(path_to_remove)\n",
    "                        except Exception as e_rem:\n",
    "                            print(f\"    Error removing file {path_to_remove}: {e_rem}\")\n",
    "                    else:\n",
    "                        print(f\"    Warning: File to remove not found: {path_to_remove}\")\n",
    "\n",
    "            except Exception as e_save:\n",
    "                print(f\"  Error saving checkpoint: {e_save}\")\n",
    "\n",
    "        # 조기 종료는 여전히 '최고 점수' 갱신 여부 기준\n",
    "        if epoch_val_f1 > max_val_f1:\n",
    "            print(f\"  Validation Macro F1 improved ({max_val_f1:.4f} --> {epoch_val_f1:.4f}).\")\n",
    "            max_val_f1 = epoch_val_f1\n",
    "            best_epoch = epoch # 최고 점수 에폭 업데이트\n",
    "            epochs_no_improve = 0 # 카운터 리셋\n",
    "        else:\n",
    "            epochs_no_improve += 1 # 최고 점수 갱신 안됨, 카운터 증가\n",
    "            print(f\"  Validation Macro F1 did not improve from the best ({max_val_f1:.4f}). ({epochs_no_improve}/{patience})\")\n",
    "\n",
    "        # 조기 종료 조건 확인\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch+1} after {patience} epochs without improvement from the best F1 score.\")\n",
    "            break # 학습 루프 종료\n",
    "\n",
    "    print(f\"\\n학습 완료.\")\n",
    "    print(f\"최종 Top-{top_k} 모델 성능 및 경로:\")\n",
    "    if not top_k_checkpoints:\n",
    "        print(\"  No models were saved.\")\n",
    "    else:\n",
    "        for i, (score, path) in enumerate(top_k_checkpoints):\n",
    "            print(f\"  Top {i+1}: Score={score:.4f}, Path={path}\")\n",
    "        # 최고 기록 자체는 여전히 max_val_f1과 best_epoch으로 알 수 있음\n",
    "        print(f\"\\nOverall Best Epoch: {best_epoch+1}, Overall Best Validation Macro F1: {max_val_f1:.4f}\")\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "# --- 3. 설정 변수 정의 ---\n",
    "MODEL_NAME = 'convnext_large.fb_in22k_ft_in1k_384' # 사용할 모델 이름\n",
    "NUM_CLASSES = 7        # 분류할 암석 종류 개수\n",
    "IMG_SIZE = 384         # 모델 입력 이미지 크기\n",
    "\n",
    "# --- 데이터 경로 설정 ---\n",
    "# TODO: 실제 데이터 경로로 수정하세요\n",
    "TRAIN_DATA_DIR = r\"/home/metaai2/workspace/limseunghwan/open/train\"\n",
    "VAL_DATA_DIR = r\"/home/metaai2/workspace/limseunghwan/open/val\"\n",
    "\n",
    "# --- 학습 하이퍼파라미터 ---\n",
    "EPOCHS = 20          # 총 학습 에폭 수 (조절 가능)\n",
    "BATCH_SIZE = 8        # 배치 크기 (GPU 메모리에 맞게 조절)\n",
    "BASE_LR = 1e-5         # 기본 학습률 (ConvNeXt fine-tuning에 적합한 값으로 시작, 튜닝 필요)\n",
    "WEIGHT_DECAY = 1e-2    # 가중치 감쇠 (AdamW와 함께 사용)\n",
    "WARMUP_EPOCHS = 5      # Warmup 에폭 수\n",
    "GRADIENT_CLIPPING = 1.0 # Gradient Clipping 값 (사용하지 않으려면 None)\n",
    "\n",
    "# --- 시스템 설정 ---\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_WORKERS = os.cpu_count() // 2  # 사용 가능한 CPU 코어의 절반 정도 (조절 가능)\n",
    "\n",
    "# --- 저장 경로 설정 ---\n",
    "SAVE_DIR = './saved_models' # 모델 저장 디렉토리\n",
    "os.makedirs(SAVE_DIR, exist_ok=True) # 디렉토리 생성\n",
    "MODEL_SAVE_PATH = os.path.join(SAVE_DIR, f'{MODEL_NAME.split(\".\")[0]}_best_f1.pth') # 모델 파일 경로\n",
    "\n",
    "# --- 4. 모델 로드 및 수정 ---\n",
    "print(f\"Loading model: {MODEL_NAME}\")\n",
    "# timm을 사용하여 사전 학습된 ConvNeXt 모델 로드\n",
    "model = timm.create_model(MODEL_NAME, pretrained=True)\n",
    "\n",
    "# 모델의 분류기(head) 부분을 새로운 클래스 수에 맞게 교체\n",
    "# num_ftrs = model.head.in_features # ConvNeXt는 보통 'head' 속성 사용\n",
    "# model.head = nn.Linear(num_ftrs, NUM_CLASSES)\n",
    "model.reset_classifier(num_classes=NUM_CLASSES)\n",
    "print(f\"Model head replaced for {NUM_CLASSES} classes.\")\n",
    "\n",
    "# 모델을 지정된 장치로 이동\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "# --- 5. 데이터 변환 정의 ---\n",
    "# 학습 데이터 변환 (Data Augmentation 포함 - 이전 예시 기반)\n",
    "# TODO: 필요시 scale, rotation, colorjitter 등 세부 파라미터 조절\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(size=(IMG_SIZE, IMG_SIZE), scale=(0.5, 1.0), ratio=(0.75, 1.3333), interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=15, interpolation=transforms.InterpolationMode.BILINEAR, fill=0),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # ImageNet 기본값 사용\n",
    "])\n",
    "\n",
    "# 검증/테스트 데이터 변환 (Data Augmentation 없음)\n",
    "# timm 기본 설정을 사용하거나 직접 정의\n",
    "try:\n",
    "    # timm 설정을 우선 사용 시도\n",
    "    config = timm.data.resolve_data_config({}, model=model)\n",
    "    val_transform = timm.data.create_transform(**config, is_training=False)\n",
    "    print(\"Using timm's default validation transform.\")\n",
    "except Exception as e:\n",
    "    # timm 설정 로드 실패 시 직접 정의\n",
    "    print(f\"Failed to get timm config ({e}), defining validation transform manually.\")\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize(IMG_SIZE, interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "        transforms.CenterCrop(IMG_SIZE),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "# --- 6. 데이터셋 및 데이터 로더 준비 ---\n",
    "# TODO: 실제 데이터셋 클래스를 사용하세요. (예: torchvision.datasets.ImageFolder)\n",
    "# ImageFolder 사용 예시:\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "print(f\"Loading datasets from: {TRAIN_DATA_DIR} and {VAL_DATA_DIR}\")\n",
    "try:\n",
    "    train_dataset = ImageFolder(root=TRAIN_DATA_DIR, transform=train_transform)\n",
    "    val_dataset = ImageFolder(root=VAL_DATA_DIR, transform=val_transform)\n",
    "\n",
    "    print(f\"Found {len(train_dataset)} training images and {len(val_dataset)} validation images.\")\n",
    "    print(f\"Classes: {train_dataset.classes}\") # 클래스 이름 확인\n",
    "\n",
    "    # Focal Loss의 alpha 계산 (선택적) - 클래스 빈도 기반\n",
    "    class_counts = np.bincount([s[1] for s in train_dataset.samples])\n",
    "    if len(class_counts) != NUM_CLASSES:\n",
    "        print(f\"Warning: Number of found classes ({len(class_counts)}) does not match NUM_CLASSES ({NUM_CLASSES}). Adjust NUM_CLASSES or check dataset.\")\n",
    "        # focal_loss_alpha = None\n",
    "    else:\n",
    "        total_samples = sum(class_counts)\n",
    "        class_weights = [total_samples / count if count > 0 else 0 for count in class_counts]\n",
    "        max_weight = max(class_weights) if any(w > 0 for w in class_weights) else 1 # 0으로 나누기 방지\n",
    "        class_weights = [w / max_weight for w in class_weights]\n",
    "        focal_loss_alpha = torch.tensor(class_weights, device=DEVICE, dtype=torch.float32)\n",
    "        print(f\"Calculated Focal Loss alpha (normalized): {focal_loss_alpha.cpu().numpy()}\")\n",
    "    # focal_loss_alpha = None # 우선 None으로 설정\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Data directory not found. Please check TRAIN_DATA_DIR and VAL_DATA_DIR.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")\n",
    "    exit()\n",
    "\n",
    "# 데이터 로더 생성\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "# --- 7. 손실 함수, 옵티마이저, 스케줄러 정의 ---\n",
    "# 손실 함수 (Focal Loss 또는 CrossEntropyLoss)\n",
    "criterion = FocalLoss(alpha=focal_loss_alpha, gamma=2.0).to(DEVICE)\n",
    "# criterion = nn.CrossEntropyLoss().to(DEVICE) # CrossEntropy 사용 시\n",
    "\n",
    "# 옵티마이저 (AdamW 추천)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=BASE_LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# LR 스케줄러 (Cosine Annealing with Warmup)\n",
    "lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS - WARMUP_EPOCHS, eta_min=BASE_LR * 0.01)\n",
    "\n",
    "# --- 8. 학습 및 검증 실행 ---\n",
    "if __name__ == \"__main__\": # 스크립트로 실행될 때만 학습 시작\n",
    "    print(\"\\nStarting training process...\")\n",
    "    model_name_base = MODEL_NAME.split('/')[-1].split('.')[0] if '/' in MODEL_NAME else MODEL_NAME.split('.')[0]\n",
    "    history = train_and_validate_best_f1(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        epochs=EPOCHS,\n",
    "        device=DEVICE,\n",
    "        num_classes=NUM_CLASSES,\n",
    "        save_dir=SAVE_DIR,             # 디렉토리 전달\n",
    "        model_name_base=model_name_base, # 모델 이름 기반 전달\n",
    "        top_k=5,                       # 저장할 개수 지정 (예: 5)\n",
    "        gradient_clipping=GRADIENT_CLIPPING,\n",
    "        lr_scheduler=lr_scheduler,\n",
    "        warmup_epochs=WARMUP_EPOCHS,\n",
    "        base_lr=BASE_LR\n",
    "    )\n",
    "\n",
    "    print(\"\\nTraining finished.\")\n",
    "    # 최종 결과 요약 출력 (함수 내에서도 출력됨)\n",
    "    if history['val_macro_f1_scores']: # 점수가 기록되었는지 확인\n",
    "        print(\"Training History Summary:\")\n",
    "        best_f1_overall = max(history['val_macro_f1_scores'])\n",
    "        best_epoch_overall = history['val_macro_f1_scores'].index(best_f1_overall) + 1\n",
    "        print(f\"  Overall Best Validation Macro F1 in history: {best_f1_overall:.4f}\")\n",
    "        print(f\"  Achieved at Epoch: {best_epoch_overall}\")\n",
    "    else:\n",
    "        print(\"No validation scores recorded in history.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dfc116b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/metaai2/miniconda3/envs/env312_cuda124_torch260/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: convnext_large.fb_in22k_ft_in1k_384\n",
      "Successfully loaded model weights from: ./saved_models/convnext_large_epoch20_f1_0.9130.pth\n",
      "Model loaded on cuda and set to evaluation mode.\n",
      "Using timm's default validation transform for inference.\n",
      "Inference Transform: Compose(\n",
      "    Resize(size=(384, 384), interpolation=bicubic, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(384, 384))\n",
      "    MaybeToTensor()\n",
      "    Normalize(mean=tensor([0.4850, 0.4560, 0.4060]), std=tensor([0.2290, 0.2240, 0.2250]))\n",
      ")\n",
      "Loading test data from: ./open/test.csv with image base: ./open\n",
      "Found 95006 images for testing.\n",
      "\n",
      "Starting inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 5938/5938 [10:45<00:00,  9.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference complete. Predictions saved to: ./submission_convnext_epoch20.csv\n",
      "Sample predictions (img_path modified):\n",
      "           ID      rock_type\n",
      "0  TEST_00000  Mud_Sandstone\n",
      "1  TEST_00001  Mud_Sandstone\n",
      "2  TEST_00002  Mud_Sandstone\n",
      "3  TEST_00003        Granite\n",
      "4  TEST_00004        Granite\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "import timm\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np # For class names later if needed\n",
    "\n",
    "# --- 0. Configuration (Adjust these paths and parameters) ---\n",
    "# Model and Data Parameters (should match training)\n",
    "MODEL_NAME = 'convnext_large.fb_in22k_ft_in1k_384' # Same as training\n",
    "NUM_CLASSES = 7        # Same as training\n",
    "IMG_SIZE = 384         # Same as training\n",
    "\n",
    "# Paths\n",
    "# TODO: IMPORTANT! Update this to the path of your BEST trained model\n",
    "SAVED_MODEL_PATH = './saved_models/convnext_large_epoch20_f1_0.9130.pth'\n",
    "TEST_CSV_PATH = './open/test.csv'    # Path to your test.csv\n",
    "IMAGE_BASE_DIR = './open'             # Base directory where images listed in test.csv are (e.g., if csv says 'test/img1.jpg', full path is IMAGE_BASE_DIR/test/img1.jpg)\n",
    "OUTPUT_CSV_PATH = './submission_convnext_epoch20.csv' # Where to save predictions\n",
    "\n",
    "# Inference Parameters\n",
    "BATCH_SIZE_INFERENCE = 16 # Adjust based on your GPU memory\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_WORKERS_INFERENCE = os.cpu_count() // 2\n",
    "\n",
    "# TODO: IMPORTANT! Define your class names in the order your model was trained.\n",
    "# You can get this from `train_dataset.classes` from your training script.\n",
    "# Example: CLASS_NAMES = ['ClassA', 'ClassB', 'ClassC', 'ClassD', 'ClassE', 'ClassF', 'ClassG']\n",
    "# If you ran the training script and printed `train_dataset.classes`, use that output here.\n",
    "# For example, if train_dataset.classes was ['andesite', 'gneiss', ...], then:\n",
    "CLASS_NAMES = ['Andesite', 'Basalt', 'Etc', 'Gneiss', 'Granite', 'Mud_Sandstone', 'Weathered_Rock'] # Replace with your actual class names in order\n",
    "\n",
    "if len(CLASS_NAMES) != NUM_CLASSES:\n",
    "    raise ValueError(f\"Number of CLASS_NAMES ({len(CLASS_NAMES)}) does not match NUM_CLASSES ({NUM_CLASSES})\")\n",
    "\n",
    "# --- 1. Model Loading ---\n",
    "print(f\"Loading model: {MODEL_NAME}\")\n",
    "model = timm.create_model(MODEL_NAME, pretrained=False, num_classes=NUM_CLASSES) # pretrained=False as we load our weights\n",
    "try:\n",
    "    model.load_state_dict(torch.load(SAVED_MODEL_PATH, map_location=DEVICE))\n",
    "    print(f\"Successfully loaded model weights from: {SAVED_MODEL_PATH}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Model file not found at {SAVED_MODEL_PATH}. Please check the path.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Could not load model weights: {e}\")\n",
    "    exit()\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "model.eval()\n",
    "print(f\"Model loaded on {DEVICE} and set to evaluation mode.\")\n",
    "\n",
    "# --- 2. Data Transformations for Inference (use validation transform from training) ---\n",
    "# Re-create the validation transform used during training.\n",
    "# If you used timm's default, try to recreate it:\n",
    "try:\n",
    "    config = timm.data.resolve_data_config({}, model=model, use_test_size=True) # use_test_size for consistency\n",
    "    # Override img_size if needed, though convnext_large_..._384 implies 384\n",
    "    config['input_size'] = (3, IMG_SIZE, IMG_SIZE) # Ensure this is (C, H, W)\n",
    "    inference_transform = timm.data.create_transform(**config, is_training=False)\n",
    "    print(\"Using timm's default validation transform for inference.\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to get timm config for inference ({e}), defining inference transform manually.\")\n",
    "    inference_transform = transforms.Compose([\n",
    "        transforms.Resize(IMG_SIZE, interpolation=transforms.InterpolationMode.BICUBIC), # Or (IMG_SIZE, IMG_SIZE)\n",
    "        transforms.CenterCrop(IMG_SIZE),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "print(f\"Inference Transform: {inference_transform}\")\n",
    "\n",
    "# --- 3. Custom Dataset for Test Images ---\n",
    "class TestImageDataset(Dataset):\n",
    "    def __init__(self, csv_path, img_dir_root, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_path (string): Path to the csv file with image paths.\n",
    "            img_dir_root (string): Root directory for images. Paths in CSV are relative to this.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.data_frame = pd.read_csv(csv_path)\n",
    "        self.img_dir_root = img_dir_root\n",
    "        self.transform = transform\n",
    "        # Assuming the CSV has a column named 'img_path' or similar for the image file paths\n",
    "        # If your column name is different, change 'img_path' below\n",
    "        if 'img_path' not in self.data_frame.columns:\n",
    "            # Fallback if 'img_path' is not present, try the first column.\n",
    "            print(f\"Warning: 'img_path' column not found in {csv_path}. Assuming first column ('{self.data_frame.columns[0]}') contains image paths.\")\n",
    "            self.img_path_column = self.data_frame.columns[0]\n",
    "        else:\n",
    "            self.img_path_column = 'img_path'\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        # img_name is the path relative to img_dir_root, e.g., \"test/test_00000.jpg\"\n",
    "        relative_img_path = self.data_frame.loc[idx, self.img_path_column]\n",
    "        # full_img_path = os.path.join(self.img_dir_root, relative_img_path)\n",
    "        # If relative_img_path already includes a base like \"test/\" and img_dir_root is \"./open\"\n",
    "        # then full path is \"./open/test/test_00000.jpg\"\n",
    "        # However, if test.csv paths are absolute or relative to project root, img_dir_root might be \"\" or \".\"\n",
    "        # Given IMAGE_BASE_DIR = './open' and CSV has 'test/test_00000.jpg',\n",
    "        # we form the path as os.path.join(IMAGE_BASE_DIR, value_from_csv_img_path_column)\n",
    "        # Let's assume paths in CSV are like 'test/image.png'\n",
    "        # and IMAGE_BASE_DIR = './open'\n",
    "        # Then the image is at './open/test/image.png'\n",
    "        # So, we use img_name = relative_img_path, because it might already be 'test/image.png'\n",
    "        # And the full path will be os.path.join(self.img_dir_root, img_name)\n",
    "\n",
    "        full_img_path = os.path.join(self.img_dir_root, relative_img_path)\n",
    "\n",
    "\n",
    "        try:\n",
    "            image = Image.open(full_img_path).convert('RGB')\n",
    "        except FileNotFoundError:\n",
    "            print(f\"ERROR: Image not found at {full_img_path}. Check IMAGE_BASE_DIR and CSV paths.\")\n",
    "            # Return a placeholder or raise error\n",
    "            # For robustness in a batch, could return a black image and handle later\n",
    "            # but for now, let's make it fail loudly if one image is missing during setup.\n",
    "            # Better: during iteration, catch and log.\n",
    "            # For __getitem__, it's often better to ensure data exists or provide a fallback.\n",
    "            # Here we let it raise if an image is critically missing.\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: Could not open image {full_img_path}: {e}\")\n",
    "            raise\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Return the image and its original filename (or path from CSV) for mapping later\n",
    "        return image, relative_img_path\n",
    "\n",
    "# --- 4. DataLoader for Test Set ---\n",
    "print(f\"Loading test data from: {TEST_CSV_PATH} with image base: {IMAGE_BASE_DIR}\")\n",
    "try:\n",
    "    test_dataset = TestImageDataset(csv_path=TEST_CSV_PATH,\n",
    "                                    img_dir_root=IMAGE_BASE_DIR,\n",
    "                                    transform=inference_transform)\n",
    "    test_loader = DataLoader(test_dataset,\n",
    "                             batch_size=BATCH_SIZE_INFERENCE,\n",
    "                             shuffle=False,\n",
    "                             num_workers=NUM_WORKERS_INFERENCE,\n",
    "                             pin_memory=True)\n",
    "    print(f\"Found {len(test_dataset)} images for testing.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Test CSV file not found at {TEST_CSV_PATH}. Please check the path.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Error creating test dataset/loader: {e}\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# --- 5. Prediction Function ---\n",
    "def predict_on_test_data(model, loader, device, class_names):\n",
    "    model.eval()\n",
    "    all_preds_indices = []\n",
    "    all_filenames = [] # To store the original filenames/paths from CSV\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, filenames_batch in tqdm(loader, desc=\"Predicting\"):\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted_indices = torch.max(outputs, 1)\n",
    "\n",
    "            all_preds_indices.extend(predicted_indices.cpu().numpy())\n",
    "            all_filenames.extend(list(filenames_batch)) # filenames_batch is a tuple of strings\n",
    "\n",
    "    # Convert predicted indices to class names\n",
    "    predicted_class_names = [class_names[idx] for idx in all_preds_indices]\n",
    "    return all_filenames, predicted_class_names\n",
    "\n",
    "# --- 6. Run Inference and Save Results ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\nStarting inference...\")\n",
    "    original_img_paths_from_loader, predictions = predict_on_test_data(model, test_loader, DEVICE, CLASS_NAMES)\n",
    "    original_test_df = pd.read_csv(TEST_CSV_PATH)\n",
    "\n",
    "    # TestImageDataset에서 사용된 img_path 컬럼 이름 가져오기\n",
    "    csv_img_path_col = test_dataset.img_path_column # 예: 'img_path'\n",
    "\n",
    "    # Loader에서 반환된 경로(original_img_paths_from_loader)와 예측값을 매핑하는 딕셔너리 생성\n",
    "    # 경로 정규화를 통해 매칭 신뢰도 향상 (os.path.normpath)\n",
    "    prediction_map = {\n",
    "        os.path.normpath(p): label\n",
    "        for p, label in zip(original_img_paths_from_loader, predictions)\n",
    "    }\n",
    "\n",
    "    # 원본 test_df의 img_path 컬럼에 대해 예측값 매핑\n",
    "    # os.path.normpath를 사용하여 CSV의 경로와 loader의 경로를 일관되게 비교\n",
    "    mapped_labels = original_test_df[csv_img_path_col].apply(\n",
    "        lambda x: prediction_map.get(os.path.normpath(x))\n",
    "    )\n",
    "\n",
    "    # 새로운 submission DataFrame 생성\n",
    "    submission_df_final = pd.DataFrame()\n",
    "\n",
    "    # img_path 컬럼: 원본 CSV의 경로에서 파일명만 추출 (확장자 제외)\n",
    "    # 예: './test/TEST_00000.jpg' -> 'TEST_00000'\n",
    "    # 예: 'test/TEST_00000.jpg' -> 'TEST_00000'\n",
    "    # 예: 'TEST_00000.jpg' -> 'TEST_00000' (만약 CSV에 이렇게만 있다면)\n",
    "    submission_df_final['ID'] = original_test_df[csv_img_path_col].apply(\n",
    "        lambda x: os.path.splitext(os.path.basename(x))[0]\n",
    "    )\n",
    "    submission_df_final['rock_type'] = mapped_labels\n",
    "\n",
    "    # 혹시 매핑되지 않은 예측값이 있는지 확인 (디버깅용)\n",
    "    if submission_df_final['rock_type'].isnull().any():\n",
    "        print(\"Warning: Some images in the CSV did not get a prediction. Check for mismatches or missing images.\")\n",
    "        # 필요한 경우 NaN 값을 기본값으로 채울 수 있습니다.\n",
    "        # 예: submission_df_final['label'].fillna(CLASS_NAMES[0], inplace=True)\n",
    "\n",
    "    # 최종 결과를 CSV 파일로 저장\n",
    "    submission_df_final.to_csv(OUTPUT_CSV_PATH, index=False)\n",
    "    print(f\"\\nInference complete. Predictions saved to: {OUTPUT_CSV_PATH}\")\n",
    "    print(f\"Sample predictions (img_path modified):\\n{submission_df_final.head()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bd33bf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9718389f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env312_cuda124_torch260",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
