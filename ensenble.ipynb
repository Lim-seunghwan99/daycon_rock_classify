{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3847b590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMG_SIZE=384에 대한 추론 변환 정의 중 (기준: MaxViT_XLarge_384_epoch19)...\n",
      "timm의 기본 변환 사용: Mean=(0.5, 0.5, 0.5), Std=(0.5, 0.5, 0.5)\n",
      "공유 추론 변환: Compose(\n",
      "    Resize(size=(384, 384), interpolation=bicubic, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(384, 384))\n",
      "    MaybeToTensor()\n",
      "    Normalize(mean=tensor([0.5000, 0.5000, 0.5000]), std=tensor([0.5000, 0.5000, 0.5000]))\n",
      ")\n",
      "\n",
      "CSV에서 테스트 데이터 로드 중: /home/metaai2/workspace/limseunghwan/open/test.csv\n",
      "CSV에서 이미지 경로에 'img_path' 열 사용 중.\n",
      "95006개의 이미지로 데이터로더를 성공적으로 생성했습니다.\n",
      "\n",
      "앙상블 추론 프로세스 시작 중...\n",
      "\n",
      "--- 모델 처리 중: MaxViT_XLarge_384_epoch19 ---\n",
      "모델 아키텍처 로드 중: maxvit_xlarge_tf_384.in21k_ft_in1k\n",
      "학습된 가중치 로드 중: ./saved_models_maxvit_xlarge_384/maxvit_xlarge_tf_384_epoch19_f1_0.9294.pth\n",
      "MaxViT_XLarge_384_epoch19의 가중치를 cuda에 성공적으로 로드했습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MaxViT_XLarge_384_epoch19으로 예측 중:  78%|███████▊  | 18568/23752 [31:40<08:50,  9.77it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 227\u001b[39m\n\u001b[32m    225\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    226\u001b[39m     model = load_model_from_config(model_conf)\n\u001b[32m--> \u001b[39m\u001b[32m227\u001b[39m     probs_np, current_paths = \u001b[43mget_probabilities\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_conf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mname\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    228\u001b[39m     all_model_probs.append(probs_np)\n\u001b[32m    230\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mweight\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model_conf:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 205\u001b[39m, in \u001b[36mget_probabilities\u001b[39m\u001b[34m(model, loader, device, model_name)\u001b[39m\n\u001b[32m    198\u001b[39m images_batch = images_batch.to(device)\n\u001b[32m    200\u001b[39m \u001b[38;5;66;03m# 모델이 예상하는 img_size가 데이터로더의 img_size와 다르면 여기서 크기 조정 (더 복잡함)\u001b[39;00m\n\u001b[32m    201\u001b[39m \u001b[38;5;66;03m# 현재는 이전 검사 / 공유 IMG_SIZE로 인해 일치한다고 가정\u001b[39;00m\n\u001b[32m    202\u001b[39m \u001b[38;5;66;03m# if model.img_size != images_batch.shape[-1]: # 확인을 위한 의사 코드\u001b[39;00m\n\u001b[32m    203\u001b[39m \u001b[38;5;66;03m#    images_batch = transforms.functional.resize(images_batch, [model.img_size, model.img_size])\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[38;5;66;03m# 소프트맥스 확률 얻기\u001b[39;00m\n\u001b[32m    207\u001b[39m probs = torch.softmax(outputs, dim=\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/env312_cuda124_torch260/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/env312_cuda124_torch260/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/env312_cuda124_torch260/lib/python3.12/site-packages/timm/models/maxxvit.py:1335\u001b[39m, in \u001b[36mMaxxVit.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m   1334\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m-> \u001b[39m\u001b[32m1335\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1336\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.forward_head(x)\n\u001b[32m   1337\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/env312_cuda124_torch260/lib/python3.12/site-packages/timm/models/maxxvit.py:1327\u001b[39m, in \u001b[36mMaxxVit.forward_features\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward_features\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m   1326\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.stem(x)\n\u001b[32m-> \u001b[39m\u001b[32m1327\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1328\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.norm(x)\n\u001b[32m   1329\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/env312_cuda124_torch260/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/env312_cuda124_torch260/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/env312_cuda124_torch260/lib/python3.12/site-packages/torch/nn/modules/container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    249\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/env312_cuda124_torch260/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/env312_cuda124_torch260/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/env312_cuda124_torch260/lib/python3.12/site-packages/timm/models/maxxvit.py:1065\u001b[39m, in \u001b[36mMaxxVitStage.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m   1063\u001b[39m     x = checkpoint_seq(\u001b[38;5;28mself\u001b[39m.blocks, x)\n\u001b[32m   1064\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1065\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mblocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1066\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/env312_cuda124_torch260/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/env312_cuda124_torch260/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/env312_cuda124_torch260/lib/python3.12/site-packages/torch/nn/modules/container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    249\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/env312_cuda124_torch260/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/env312_cuda124_torch260/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/env312_cuda124_torch260/lib/python3.12/site-packages/timm/models/maxxvit.py:955\u001b[39m, in \u001b[36mMaxxVitBlock.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    953\u001b[39m     x = x.permute(\u001b[32m0\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# to NHWC (channels-last)\u001b[39;00m\n\u001b[32m    954\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.attn_block \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m955\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattn_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    956\u001b[39m x = \u001b[38;5;28mself\u001b[39m.attn_grid(x)\n\u001b[32m    957\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.nchw_attn:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/env312_cuda124_torch260/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/env312_cuda124_torch260/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ensemble_inference.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "import timm\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. 설정 (Configuration) ---\n",
    "# --- 공통 파라미터 ---\n",
    "NUM_CLASSES = 7        # 모델 학습 시 사용된 클래스 수\n",
    "# CRITICAL! 학습 시 순서와 정확히 일치해야 함\n",
    "CLASS_NAMES = ['Andesite', 'Basalt', 'Etc', 'Gneiss', 'Granite', 'Mud_Sandstone', 'Weathered_Rock']\n",
    "if len(CLASS_NAMES) != NUM_CLASSES:\n",
    "    raise ValueError(f\"CLASS_NAMES의 개수({len(CLASS_NAMES)})가 NUM_CLASSES({NUM_CLASSES})와 일치하지 않습니다.\")\n",
    "\n",
    "TEST_CSV_PATH = r'/home/metaai2/workspace/limseunghwan/open/test.csv'    # test.csv 경로\n",
    "IMAGE_BASE_DIR = r'/home/metaai2/workspace/limseunghwan/open'           # 이미지 기본 디렉토리\n",
    "OUTPUT_CSV_DIR = './submissions_ensemble' # 출력 CSV 저장 디렉토리\n",
    "\n",
    "# --- 추론 파라미터 ---\n",
    "# BATCH_SIZE_INFERENCE는 모델 중 가장 큰 모델에 맞춰 결정됩니다.\n",
    "# 여기서는 공통된 값을 사용합니다. 모델 크기 차이가 크면 조정하세요.\n",
    "BATCH_SIZE_INFERENCE = 4 # GPU 메모리에 따라 조정 (가장 큰 모델 기준)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_WORKERS_INFERENCE = os.cpu_count() // 2 if os.cpu_count() else 4\n",
    "\n",
    "# --- 모델별 설정 ---\n",
    "# TODO: 중요! 실제 모델 세부 정보로 업데이트하세요.\n",
    "MODEL_CONFIGS = [\n",
    "    {\n",
    "        \"name\": \"MaxViT_XLarge_384_epoch19\", # 이 모델에 대한 설명적인 이름\n",
    "        \"architecture\": 'maxvit_xlarge_tf_384.in21k_ft_in1k',\n",
    "        \"saved_path\": './saved_models_maxvit_xlarge_384/maxvit_xlarge_tf_384_epoch19_f1_0.9294.pth',\n",
    "        \"img_size\": 384,\n",
    "        \"weight\": 0.65 # 선택 사항: 소프트 보팅 시 가중 평균용\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"ConvNeXt_Large_384_epoch20\",\n",
    "        \"architecture\": 'convnext_large.fb_in22k_ft_in1k_384',\n",
    "        \"saved_path\": './saved_models_convnext/convnext_large_epoch19_f1_0.9121.pth', # 예시 경로!\n",
    "        \"img_size\": 384,\n",
    "        \"weight\": 0.5 # 선택 사항\n",
    "    },\n",
    "    # 필요한 경우 여기에 더 많은 모델 설정을 추가하세요.\n",
    "]\n",
    "\n",
    "# --- 무결성 검사: 모든 모델은 단일 데이터로더를 위해 동일한 이미지 크기를 사용하는 것이 이상적입니다 ---\n",
    "# IMG_SIZE가 다르면 여러 데이터로더를 사용하거나 모델별로 즉석에서 크기를 조정하는 등 더 복잡한 설정이 필요합니다.\n",
    "# 이 스크립트에서는 데이터로더를 위해 목록의 *첫 번째* 모델의 IMG_SIZE를 사용한다고 가정합니다.\n",
    "# MODEL_CONFIGS의 모든 `img_size`가 동일한 것이 가장 좋습니다.\n",
    "first_model_img_size = MODEL_CONFIGS[0][\"img_size\"]\n",
    "for config in MODEL_CONFIGS:\n",
    "    if config[\"img_size\"] != first_model_img_size:\n",
    "        print(f\"경고: 모델 {config['name']}의 img_size({config['img_size']})가 첫 번째 모델의 img_size({first_model_img_size})와 다릅니다. \"\n",
    "              f\"데이터로더는 {first_model_img_size}를 사용합니다. 이로 인해 {config['name']}의 성능이 저하될 수 있습니다.\")\n",
    "        # 이것이 의도된 것이 아니라면 오류를 발생시키거나 여러 데이터로더를 구현하는 것을 고려하십시오.\n",
    "IMG_SIZE = first_model_img_size # 데이터로더에는 첫 번째 모델의 이미지 크기를 사용\n",
    "\n",
    "# --- 2. 추론용 데이터 변환 (Data Transformations for Inference) ---\n",
    "# 하나의 변환을 만듭니다. 이상적으로는 모델 중 하나 또는 일반적인 변환을 기반으로 합니다.\n",
    "# 목록의 첫 번째 모델에 대해 timm의 권장 설정을 사용해 보겠습니다.\n",
    "print(f\"IMG_SIZE={IMG_SIZE}에 대한 추론 변환 정의 중 (기준: {MODEL_CONFIGS[0]['name']})...\")\n",
    "try:\n",
    "    # 공유 데이터로더를 위한 '모델 인식' 변환을 얻기 위한 약간의 편법으로\n",
    "    # 첫 번째 유형의 더미 모델 인스턴스를 만들어 해당 data_config를 가져옵니다.\n",
    "    temp_model_for_config = timm.create_model(\n",
    "        MODEL_CONFIGS[0][\"architecture\"],\n",
    "        pretrained=True, # 기본 설정을 로드하기 위해 pretrained=True 사용\n",
    "        num_classes=NUM_CLASSES, # 설정에는 필수는 아니지만 좋은 습관\n",
    "        img_size=IMG_SIZE\n",
    "    )\n",
    "    config = timm.data.resolve_data_config({}, model=temp_model_for_config)\n",
    "    config['input_size'] = (3, IMG_SIZE, IMG_SIZE) # (C, H, W)\n",
    "    inference_transform = timm.data.create_transform(**config, is_training=False)\n",
    "    print(f\"timm의 기본 변환 사용: Mean={config['mean']}, Std={config['std']}\")\n",
    "    del temp_model_for_config # 정리\n",
    "except Exception as e:\n",
    "    print(f\"첫 번째 모델에 대한 timm 설정 가져오기 실패 ({e}). ImageNet 기본값을 사용하여 수동으로 변환 정의 중.\")\n",
    "    inference_transform = transforms.Compose([\n",
    "        transforms.Resize(IMG_SIZE, interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "        transforms.CenterCrop(IMG_SIZE),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "print(f\"공유 추론 변환: {inference_transform}\")\n",
    "\n",
    "\n",
    "# --- 3. 테스트 이미지를 위한 사용자 정의 데이터셋 및 데이터로더 ---\n",
    "class TestImageDataset(Dataset):\n",
    "    def __init__(self, csv_path, img_dir_root, transform=None):\n",
    "        self.data_frame = pd.read_csv(csv_path)\n",
    "        self.img_dir_root = img_dir_root\n",
    "        self.transform = transform\n",
    "        if 'img_path' not in self.data_frame.columns:\n",
    "            print(f\"경고: {csv_path}에서 'img_path' 열을 찾을 수 없습니다. 첫 번째 열('{self.data_frame.columns[0]}')에 이미지 경로가 있다고 가정합니다.\")\n",
    "            self.img_path_column = self.data_frame.columns[0]\n",
    "        else:\n",
    "            self.img_path_column = 'img_path'\n",
    "        print(f\"CSV에서 이미지 경로에 '{self.img_path_column}' 열 사용 중.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        relative_img_path = self.data_frame.loc[idx, self.img_path_column]\n",
    "        full_img_path = os.path.join(self.img_dir_root, relative_img_path)\n",
    "        try:\n",
    "            image = Image.open(full_img_path).convert('RGB')\n",
    "        except FileNotFoundError:\n",
    "            print(f\"오류: {full_img_path}에서 이미지를 찾을 수 없습니다.\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            print(f\"오류: {full_img_path} 이미지를 열 수 없습니다: {e}\")\n",
    "            raise\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, relative_img_path\n",
    "\n",
    "print(f\"\\nCSV에서 테스트 데이터 로드 중: {TEST_CSV_PATH}\")\n",
    "try:\n",
    "    test_dataset = TestImageDataset(csv_path=TEST_CSV_PATH,\n",
    "                                    img_dir_root=IMAGE_BASE_DIR,\n",
    "                                    transform=inference_transform)\n",
    "    if len(test_dataset) == 0:\n",
    "        print(f\"오류: {TEST_CSV_PATH}에서 이미지를 찾을 수 없습니다.\")\n",
    "        exit()\n",
    "    test_loader = DataLoader(test_dataset,\n",
    "                             batch_size=BATCH_SIZE_INFERENCE,\n",
    "                             shuffle=False,\n",
    "                             num_workers=NUM_WORKERS_INFERENCE,\n",
    "                             pin_memory=True)\n",
    "    print(f\"{len(test_dataset)}개의 이미지로 데이터로더를 성공적으로 생성했습니다.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"오류: {TEST_CSV_PATH}에서 테스트 CSV 파일을 찾을 수 없습니다.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"오류: 테스트 데이터셋 또는 데이터로더를 생성할 수 없습니다: {e}\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# --- 4. 모델 로딩 및 예측 함수 ---\n",
    "def load_model_from_config(model_conf):\n",
    "    print(f\"모델 아키텍처 로드 중: {model_conf['architecture']}\")\n",
    "\n",
    "    # 모델 아키텍처에 따라 img_size 인자 전달 여부 결정\n",
    "    if 'maxvit' in model_conf['architecture'].lower(): # MaxViT 계열 모델인 경우\n",
    "        model = timm.create_model(\n",
    "            model_conf['architecture'],\n",
    "            pretrained=False,\n",
    "            num_classes=NUM_CLASSES,\n",
    "            img_size=model_conf['img_size'] # MaxViT는 img_size를 받음\n",
    "        )\n",
    "    elif 'convnext' in model_conf['architecture'].lower(): # ConvNeXt 계열 모델인 경우\n",
    "        model = timm.create_model(\n",
    "            model_conf['architecture'],\n",
    "            pretrained=False,\n",
    "            num_classes=NUM_CLASSES\n",
    "            # ConvNeXt는 일반적으로 img_size 인자를 직접 받지 않음\n",
    "        )\n",
    "    else: # 다른 모델 아키텍처의 경우 (기본적으로 img_size를 전달하지 않거나, 필요시 추가 조건)\n",
    "        print(f\"주의: {model_conf['architecture']} 아키텍처에 대한 img_size 처리 규칙이 명시되지 않았습니다. img_size 인자 없이 로드를 시도합니다.\")\n",
    "        # 대부분의 timm 모델은 img_size를 필수로 요구하지 않거나, 아키텍처 이름에 크기가 포함됨.\n",
    "        # 특정 모델이 img_size를 요구하고 위 조건에 해당하지 않는다면, 여기에 elif로 추가해야 함.\n",
    "        # 예: elif 'some_other_model_prefix' in model_conf['architecture'].lower():\n",
    "        #         model = timm.create_model(..., img_size=model_conf['img_size'])\n",
    "        model = timm.create_model(\n",
    "            model_conf['architecture'],\n",
    "            pretrained=False,\n",
    "            num_classes=NUM_CLASSES\n",
    "        )\n",
    "\n",
    "    print(f\"학습된 가중치 로드 중: {model_conf['saved_path']}\")\n",
    "    if not os.path.exists(model_conf['saved_path']):\n",
    "        print(f\"오류: 모델 가중치 파일 {model_conf['saved_path']}을(를) 찾을 수 없습니다.\") # 경로 변수명 수정\n",
    "        raise FileNotFoundError(f\"모델 가중치를 찾을 수 없음: {model_conf['saved_path']}\")\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(model_conf['saved_path'], map_location=DEVICE))\n",
    "        print(f\"{model_conf['name']}의 가중치를 {DEVICE}에 성공적으로 로드했습니다.\")\n",
    "    except Exception as e:\n",
    "        print(f\"오류: {model_conf['name']}의 가중치를 로드할 수 없습니다: {e}\")\n",
    "        raise\n",
    "    model = model.to(DEVICE)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def get_probabilities(model, loader, device, model_name=\"Model\"):\n",
    "    model.eval()\n",
    "    all_probs_list = []\n",
    "    all_original_filenames_list = [] # 순서와 내용 일관성 보장을 위해\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images_batch, filenames_batch in tqdm(loader, desc=f\"{model_name}으로 예측 중\"):\n",
    "            images_batch = images_batch.to(device)\n",
    "\n",
    "            # 모델이 예상하는 img_size가 데이터로더의 img_size와 다르면 여기서 크기 조정 (더 복잡함)\n",
    "            # 현재는 이전 검사 / 공유 IMG_SIZE로 인해 일치한다고 가정\n",
    "            # if model.img_size != images_batch.shape[-1]: # 확인을 위한 의사 코드\n",
    "            #    images_batch = transforms.functional.resize(images_batch, [model.img_size, model.img_size])\n",
    "\n",
    "            outputs = model(images_batch)\n",
    "            # 소프트맥스 확률 얻기\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            all_probs_list.append(probs.cpu()) # CPU에 저장\n",
    "            all_original_filenames_list.extend(list(filenames_batch))\n",
    "\n",
    "    all_probs_tensor = torch.cat(all_probs_list, dim=0)\n",
    "    return all_probs_tensor.numpy(), all_original_filenames_list\n",
    "\n",
    "\n",
    "# --- 5. 모든 모델에 대한 추론 실행 및 앙상블 ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n앙상블 추론 프로세스 시작 중...\")\n",
    "\n",
    "    all_model_probs = [] # 각 모델의 확률 배열(이미지 수 x 클래스 수)을 저장할 리스트\n",
    "    model_weights = []   # 가중 평균용 (지정된 경우)\n",
    "    image_paths_from_loader = None # 첫 번째 모델 실행에서 이미지 경로를 저장하기 위함\n",
    "\n",
    "    for i, model_conf in enumerate(MODEL_CONFIGS):\n",
    "        print(f\"\\n--- 모델 처리 중: {model_conf['name']} ---\")\n",
    "        try:\n",
    "            model = load_model_from_config(model_conf)\n",
    "            probs_np, current_paths = get_probabilities(model, test_loader, DEVICE, model_conf['name'])\n",
    "            all_model_probs.append(probs_np)\n",
    "\n",
    "            if 'weight' in model_conf:\n",
    "                model_weights.append(model_conf['weight'])\n",
    "            else:\n",
    "                model_weights.append(1.0) # 기본 가중치는 1.0\n",
    "\n",
    "            if image_paths_from_loader is None:\n",
    "                image_paths_from_loader = current_paths\n",
    "            elif image_paths_from_loader != current_paths:\n",
    "                # 데이터로더가 일관적이라면 이상적으로는 발생하지 않아야 함\n",
    "                print(\"경고: 모델 예측 간 이미지 경로 순서 불일치. 문제가 있을 수 있습니다.\")\n",
    "                # 견고성을 위해 경로를 기반으로 재정렬을 시도할 수 있지만 복잡합니다.\n",
    "                # 현재는 데이터로더의 순서가 항상 동일하다고 가정합니다.\n",
    "                pass\n",
    "            del model # 메모리 해제\n",
    "            torch.cuda.empty_cache() # CUDA 캐시 비우기\n",
    "        except Exception as e:\n",
    "            print(f\"오류: 모델 {model_conf['name']} 처리 중 오류 발생: {e}. 이 모델은 앙상블에서 제외합니다.\")\n",
    "            # 선택적으로, 오류 전에 가중치가 추가된 경우 해당 가중치를 제거하거나 나중에 처리\n",
    "            if len(model_weights) > len(all_model_probs):\n",
    "                model_weights.pop()\n",
    "\n",
    "\n",
    "    if not all_model_probs:\n",
    "        print(\"오류: 성공적으로 처리된 모델이 없습니다. 앙상블을 생성할 수 없습니다. 종료합니다.\")\n",
    "        exit()\n",
    "\n",
    "    # --- 앙상블 예측 (소프트 보팅, 선택적 가중치 사용) ---\n",
    "    print(\"\\n소프트 보팅을 사용하여 예측 앙상블 중...\")\n",
    "\n",
    "    # 가중치 정규화 (이미 합이 1이 아니면 합이 1이 되도록)\n",
    "    total_weight = sum(model_weights)\n",
    "    if total_weight == 0: # 적어도 하나의 모델이 성공했다면 발생하지 않아야 함\n",
    "        print(\"경고: 총 가중치가 0입니다. 동일한 가중치를 사용합니다.\")\n",
    "        normalized_weights = [1.0/len(all_model_probs)] * len(all_model_probs)\n",
    "    else:\n",
    "        normalized_weights = [w / total_weight for w in model_weights]\n",
    "\n",
    "    # 확률의 가중 평균\n",
    "    # all_model_probs가 numpy 배열인지 확인\n",
    "    ensembled_probs_sum = np.zeros_like(all_model_probs[0]) # 첫 번째 모델 확률의 형태로 초기화\n",
    "    for i, probs_array in enumerate(all_model_probs):\n",
    "        ensembled_probs_sum += probs_array * normalized_weights[i]\n",
    "\n",
    "    # 앙상블된 확률에서 최종 예측 인덱스\n",
    "    final_predicted_indices = np.argmax(ensembled_probs_sum, axis=1)\n",
    "\n",
    "    # 인덱스를 클래스 이름으로 변환\n",
    "    ensembled_class_names = [CLASS_NAMES[idx] for idx in final_predicted_indices]\n",
    "\n",
    "    # --- 6. 제출 파일 생성 및 저장 ---\n",
    "    print(\"\\n제출 파일 준비 중...\")\n",
    "    original_test_df = pd.read_csv(TEST_CSV_PATH)\n",
    "\n",
    "    # 데이터로더에서 반환된 이미지 경로와 앙상블된 예측을 매핑\n",
    "    prediction_map = {\n",
    "        os.path.normpath(path): pred_label\n",
    "        for path, pred_label in zip(image_paths_from_loader, ensembled_class_names)\n",
    "    }\n",
    "\n",
    "    csv_img_path_col = test_dataset.img_path_column\n",
    "    mapped_predictions = original_test_df[csv_img_path_col].apply(\n",
    "        lambda x: prediction_map.get(os.path.normpath(x))\n",
    "    )\n",
    "\n",
    "    submission_df = pd.DataFrame()\n",
    "    submission_df['ID'] = original_test_df[csv_img_path_col].apply(\n",
    "        lambda x: os.path.splitext(os.path.basename(x))[0]\n",
    "    )\n",
    "    submission_df['rock_type'] = mapped_predictions\n",
    "\n",
    "    if submission_df['rock_type'].isnull().any():\n",
    "        num_null = submission_df['rock_type'].isnull().sum()\n",
    "        print(f\"경고: CSV의 {num_null}개 이미지가 예측을 받지 못했습니다. \"\n",
    "              \"누락된 이미지 파일 또는 매핑 중 오류 때문일 수 있습니다.\")\n",
    "        # 필요한 경우 NaN 값을 채웁니다. 예: 'Etc' 클래스 또는 가장 빈번한 클래스 (허용되는 경우)\n",
    "        # submission_df['rock_type'].fillna('Etc', inplace=True)\n",
    "        # print(f\"{num_null}개의 NaN을 'Etc'로 채웠습니다.\")\n",
    "\n",
    "    os.makedirs(OUTPUT_CSV_DIR, exist_ok=True)\n",
    "    # 성공적으로 로드된 모델의 이름만 사용\n",
    "    ensemble_model_names_list = []\n",
    "    for cfg in MODEL_CONFIGS:\n",
    "        # Check if model was successfully processed by seeing if its probs are in all_model_probs\n",
    "        # This requires matching by some unique identifier, e.g. its original config index or name if names are unique\n",
    "        # For simplicity, let's assume order in MODEL_CONFIGS matches order in all_model_probs if no failures\n",
    "        # Or, more robustly, only include names of models whose paths exist and were attempted.\n",
    "        if os.path.exists(cfg[\"saved_path\"]): # Simple check, assumes path existence means it was part of the attempt\n",
    "            cleaned_name = cfg[\"name\"].split('_epoch')[0].replace('_384','')\n",
    "            ensemble_model_names_list.append(cleaned_name)\n",
    "\n",
    "    ensemble_model_names = \"_\".join(ensemble_model_names_list)\n",
    "\n",
    "    if not all_model_probs: # 모든 모델 로딩 실패 시\n",
    "        ensemble_model_names = \"ENSEMBLE_FAILED\"\n",
    "    elif len(all_model_probs) < len(MODEL_CONFIGS): # 일부 모델만 성공\n",
    "         ensemble_model_names += \"_PARTIAL\"\n",
    "\n",
    "\n",
    "    output_csv_filename = f\"submission_ensemble_{ensemble_model_names}.csv\"\n",
    "    final_output_csv_path = os.path.join(OUTPUT_CSV_DIR, output_csv_filename)\n",
    "\n",
    "    submission_df.to_csv(final_output_csv_path, index=False)\n",
    "    print(f\"\\n앙상블 추론 완료. 예측 결과 저장 위치: {final_output_csv_path}\")\n",
    "    print(f\"제출 파일 샘플:\\n{submission_df.head()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68cf6bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/metaai2/miniconda3/envs/env312_cuda124_torch260/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMG_SIZE=384에 대한 추론 변환 정의 중 (기준: MaxViT_XLarge_384_epoch19)...\n",
      "timm의 기본 변환 사용: Mean=(0.5, 0.5, 0.5), Std=(0.5, 0.5, 0.5), Input_Size=(3, 384, 384)\n",
      "공유 추론 변환: Compose(\n",
      "    Resize(size=(384, 384), interpolation=bicubic, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(384, 384))\n",
      "    MaybeToTensor()\n",
      "    Normalize(mean=tensor([0.5000, 0.5000, 0.5000]), std=tensor([0.5000, 0.5000, 0.5000]))\n",
      ")\n",
      "\n",
      "CSV에서 테스트 데이터 로드 중: /home/metaai2/workspace/limseunghwan/open/test.csv\n",
      "CSV에서 이미지 경로에 'img_path' 열 사용 중.\n",
      "95006개의 이미지로 데이터로더를 성공적으로 생성했습니다.\n",
      "\n",
      "앙상블 추론 프로세스 시작 중...\n",
      "사용 디바이스: cuda\n",
      "\n",
      "--- 모델 처리 중: MaxViT_XLarge_384_epoch19 ---\n",
      "모델 아키텍처 로드 중: maxvit_xlarge_tf_384.in21k_ft_in1k\n",
      "학습된 가중치 로드 중: ./saved_models_maxvit_xlarge_384/maxvit_xlarge_tf_384_epoch19_f1_0.9294.pth\n",
      "MaxViT_XLarge_384_epoch19의 가중치를 성공적으로 로드했습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MaxViT_XLarge_384_epoch19으로 예측 중: 100%|██████████| 23752/23752 [40:24<00:00,  9.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 모델 처리 중: ConvNeXt_Large_384_epoch20 ---\n",
      "모델 아키텍처 로드 중: convnext_large.fb_in22k_ft_in1k_384\n",
      "학습된 가중치 로드 중: ./saved_models_convnext/convnext_large_epoch19_f1_0.9121.pth\n",
      "ConvNeXt_Large_384_epoch20의 가중치를 성공적으로 로드했습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ConvNeXt_Large_384_epoch20으로 예측 중: 100%|██████████| 23752/23752 [11:46<00:00, 33.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 모델 처리 중: Swin_Large_384_Best ---\n",
      "모델 아키텍처 로드 중: swin_large_patch4_window12_384.ms_in22k_ft_in1k\n",
      "학습된 가중치 로드 중: ./best_swin_large_model_test_limited_batches.pth\n",
      "Swin_Large_384_Best의 가중치를 성공적으로 로드했습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Swin_Large_384_Best으로 예측 중: 100%|██████████| 23752/23752 [15:21<00:00, 25.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "소프트 보팅을 사용하여 예측 앙상블 중...\n",
      "앙상블에 사용된 모델 및 가중치:\n",
      "- 모델: MaxViT_XLarge_384_epoch19, 정규화된 가중치: 0.4000\n",
      "- 모델: ConvNeXt_Large_384_epoch20, 정규화된 가중치: 0.2667\n",
      "- 모델: Swin_Large_384_Best, 정규화된 가중치: 0.3333\n",
      "\n",
      "제출 파일 준비 중...\n",
      "\n",
      "앙상블 추론 완료. 예측 결과 저장 위치: ./submissions_ensemble/submission_ensemble_ConvNeXt_Large20_MaxViT_XLarge19_Swin_Large_Best.csv\n",
      "제출 파일 샘플:\n",
      "           ID      rock_type\n",
      "0  TEST_00000  Mud_Sandstone\n",
      "1  TEST_00001  Mud_Sandstone\n",
      "2  TEST_00002  Mud_Sandstone\n",
      "3  TEST_00003        Granite\n",
      "4  TEST_00004        Granite\n"
     ]
    }
   ],
   "source": [
    "# ensemble_inference.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "import timm\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. 설정 (Configuration) ---\n",
    "# --- 공통 파라미터 ---\n",
    "NUM_CLASSES = 7        # 모델 학습 시 사용된 클래스 수\n",
    "# CRITICAL! 학습 시 순서와 정확히 일치해야 함\n",
    "CLASS_NAMES = ['Andesite', 'Basalt', 'Etc', 'Gneiss', 'Granite', 'Mud_Sandstone', 'Weathered_Rock']\n",
    "if len(CLASS_NAMES) != NUM_CLASSES:\n",
    "    raise ValueError(f\"CLASS_NAMES의 개수({len(CLASS_NAMES)})가 NUM_CLASSES({NUM_CLASSES})와 일치하지 않습니다.\")\n",
    "\n",
    "TEST_CSV_PATH = r'/home/metaai2/workspace/limseunghwan/open/test.csv'    # test.csv 경로\n",
    "IMAGE_BASE_DIR = r'/home/metaai2/workspace/limseunghwan/open'           # 이미지 기본 디렉토리\n",
    "OUTPUT_CSV_DIR = './submissions_ensemble' # 출력 CSV 저장 디렉토리\n",
    "\n",
    "# --- 추론 파라미터 ---\n",
    "BATCH_SIZE_INFERENCE = 4 # GPU 메모리에 따라 조정 (가장 큰 모델 기준)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_WORKERS_INFERENCE = os.cpu_count() // 2 if os.cpu_count() is not None else 4 # os.cpu_count() None 반환 가능성 처리\n",
    "\n",
    "# --- 모델별 설정 ---\n",
    "MODEL_CONFIGS = [\n",
    "    {\n",
    "        \"name\": \"MaxViT_XLarge_384_epoch19\",\n",
    "        \"architecture\": 'maxvit_xlarge_tf_384.in21k_ft_in1k',\n",
    "        \"saved_path\": './saved_models_maxvit_xlarge_384/maxvit_xlarge_tf_384_epoch19_f1_0.9294.pth',\n",
    "        \"img_size\": 384,\n",
    "        \"weight\": 0.75\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"ConvNeXt_Large_384_epoch20\",\n",
    "        \"architecture\": 'convnext_large.fb_in22k_ft_in1k_384',\n",
    "        \"saved_path\": './saved_models_convnext/convnext_large_epoch19_f1_0.9121.pth',\n",
    "        \"img_size\": 384,\n",
    "        \"weight\": 0.5\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Swin_Large_384_Best\", # Swin Transformer 모델 이름\n",
    "        \"architecture\": 'swin_large_patch4_window12_384.ms_in22k_ft_in1k', # Swin Transformer 아키텍처\n",
    "        \"saved_path\": './best_swin_large_model_test_limited_batches.pth', # Swin Transformer 가중치 경로\n",
    "        \"img_size\": 384, # Swin Transformer 이미지 크기\n",
    "        \"weight\": 0.5 # Swin Transformer 앙상블 가중치 (예시, 조정 필요)\n",
    "    }\n",
    "]\n",
    "\n",
    "# --- 무결성 검사: 모든 모델은 단일 데이터로더를 위해 동일한 이미지 크기를 사용하는 것이 이상적입니다 ---\n",
    "first_model_img_size = MODEL_CONFIGS[0][\"img_size\"]\n",
    "for config in MODEL_CONFIGS:\n",
    "    if config[\"img_size\"] != first_model_img_size:\n",
    "        print(f\"경고: 모델 {config['name']}의 img_size({config['img_size']})가 첫 번째 모델의 img_size({first_model_img_size})와 다릅니다. \"\n",
    "              f\"데이터로더는 {first_model_img_size}를 사용합니다. 이로 인해 {config['name']}의 성능이 저하될 수 있습니다.\")\n",
    "IMG_SIZE = first_model_img_size\n",
    "\n",
    "# --- 2. 추론용 데이터 변환 (Data Transformations for Inference) ---\n",
    "print(f\"IMG_SIZE={IMG_SIZE}에 대한 추론 변환 정의 중 (기준: {MODEL_CONFIGS[0]['name']})...\")\n",
    "try:\n",
    "    temp_model_for_config = timm.create_model(\n",
    "        MODEL_CONFIGS[0][\"architecture\"],\n",
    "        pretrained=True,\n",
    "        num_classes=NUM_CLASSES,\n",
    "        img_size=IMG_SIZE # img_size 명시적 전달 (MaxViT의 경우 필요할 수 있음)\n",
    "    )\n",
    "    data_config = timm.data.resolve_data_config({}, model=temp_model_for_config)\n",
    "    # data_config['input_size'] = (3, IMG_SIZE, IMG_SIZE) # timm이 알아서 설정하므로 중복될 수 있음\n",
    "    inference_transform = timm.data.create_transform(**data_config, is_training=False)\n",
    "    print(f\"timm의 기본 변환 사용: Mean={data_config['mean']}, Std={data_config['std']}, Input_Size={data_config['input_size']}\")\n",
    "    del temp_model_for_config\n",
    "except Exception as e:\n",
    "    print(f\"첫 번째 모델에 대한 timm 설정 가져오기 실패 ({e}). ImageNet 기본값을 사용하여 수동으로 변환 정의 중.\")\n",
    "    inference_transform = transforms.Compose([\n",
    "        transforms.Resize(IMG_SIZE, interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "        transforms.CenterCrop(IMG_SIZE),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "print(f\"공유 추론 변환: {inference_transform}\")\n",
    "\n",
    "\n",
    "# --- 3. 테스트 이미지를 위한 사용자 정의 데이터셋 및 데이터로더 ---\n",
    "class TestImageDataset(Dataset):\n",
    "    def __init__(self, csv_path, img_dir_root, transform=None):\n",
    "        self.data_frame = pd.read_csv(csv_path)\n",
    "        self.img_dir_root = img_dir_root\n",
    "        self.transform = transform\n",
    "        if 'img_path' not in self.data_frame.columns:\n",
    "            print(f\"경고: {csv_path}에서 'img_path' 열을 찾을 수 없습니다. 첫 번째 열('{self.data_frame.columns[0]}')에 이미지 경로가 있다고 가정합니다.\")\n",
    "            self.img_path_column = self.data_frame.columns[0]\n",
    "        else:\n",
    "            self.img_path_column = 'img_path'\n",
    "        print(f\"CSV에서 이미지 경로에 '{self.img_path_column}' 열 사용 중.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        relative_img_path = self.data_frame.loc[idx, self.img_path_column]\n",
    "        full_img_path = os.path.join(self.img_dir_root, relative_img_path)\n",
    "        try:\n",
    "            image = Image.open(full_img_path).convert('RGB')\n",
    "        except FileNotFoundError:\n",
    "            print(f\"오류: {full_img_path}에서 이미지를 찾을 수 없습니다.\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            print(f\"오류: {full_img_path} 이미지를 열 수 없습니다: {e}\")\n",
    "            raise\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, relative_img_path\n",
    "\n",
    "print(f\"\\nCSV에서 테스트 데이터 로드 중: {TEST_CSV_PATH}\")\n",
    "try:\n",
    "    test_dataset = TestImageDataset(csv_path=TEST_CSV_PATH,\n",
    "                                    img_dir_root=IMAGE_BASE_DIR,\n",
    "                                    transform=inference_transform)\n",
    "    if len(test_dataset) == 0:\n",
    "        print(f\"오류: {TEST_CSV_PATH}에서 이미지를 찾을 수 없습니다.\")\n",
    "        exit()\n",
    "    test_loader = DataLoader(test_dataset,\n",
    "                             batch_size=BATCH_SIZE_INFERENCE,\n",
    "                             shuffle=False,\n",
    "                             num_workers=NUM_WORKERS_INFERENCE,\n",
    "                             pin_memory=True)\n",
    "    print(f\"{len(test_dataset)}개의 이미지로 데이터로더를 성공적으로 생성했습니다.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"오류: {TEST_CSV_PATH}에서 테스트 CSV 파일을 찾을 수 없습니다.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"오류: 테스트 데이터셋 또는 데이터로더를 생성할 수 없습니다: {e}\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# --- 4. 모델 로딩 및 예측 함수 ---\n",
    "def load_model_from_config(model_conf):\n",
    "    print(f\"모델 아키텍처 로드 중: {model_conf['architecture']}\")\n",
    "    architecture_lower = model_conf['architecture'].lower()\n",
    "\n",
    "    # img_size를 필요로 하는 모델이나, 아키텍처 문자열에 크기가 명시되지 않은 경우를 위한 처리\n",
    "    # timm 대부분 모델은 아키텍처 이름에 크기가 명시되어 있으면 img_size 인자가 필수는 아님.\n",
    "    # 하지만 명시적으로 전달하는 것이 더 안전할 수 있음.\n",
    "    if 'maxvit' in architecture_lower : # MaxViT는 img_size를 명시적으로 받는 것이 좋음\n",
    "        model = timm.create_model(\n",
    "            model_conf['architecture'],\n",
    "            pretrained=False,\n",
    "            num_classes=NUM_CLASSES,\n",
    "            img_size=model_conf['img_size']\n",
    "        )\n",
    "    # ConvNeXt의 경우 아키텍처 이름에 img_size가 포함되어 있음 (e.g., convnext_large_in22k_ft_in1k_384)\n",
    "    # Swin Transformer의 경우도 아키텍처 이름에 img_size가 포함되어 있음 (e.g., swin_large_patch4_window12_384)\n",
    "    # 따라서 이들은 img_size 인자 없이도 잘 로드될 수 있음.\n",
    "    elif 'convnext' in architecture_lower or 'swin' in architecture_lower:\n",
    "         model = timm.create_model(\n",
    "            model_conf['architecture'],\n",
    "            pretrained=False,\n",
    "            num_classes=NUM_CLASSES\n",
    "            # img_size=model_conf['img_size'] # 필요하다면 추가할 수 있으나, 보통 아키텍처 이름으로 충분\n",
    "        )\n",
    "    else:\n",
    "        print(f\"주의: {model_conf['architecture']} 아키텍처에 대한 img_size 처리 규칙이 명시되지 않았습니다. img_size 인자 없이 로드를 시도합니다.\")\n",
    "        model = timm.create_model(\n",
    "            model_conf['architecture'],\n",
    "            pretrained=False,\n",
    "            num_classes=NUM_CLASSES\n",
    "        )\n",
    "\n",
    "    print(f\"학습된 가중치 로드 중: {model_conf['saved_path']}\")\n",
    "    if not os.path.exists(model_conf['saved_path']):\n",
    "        print(f\"오류: 모델 가중치 파일 {model_conf['saved_path']}을(를) 찾을 수 없습니다.\")\n",
    "        raise FileNotFoundError(f\"모델 가중치를 찾을 수 없음: {model_conf['saved_path']}\")\n",
    "    try:\n",
    "        # 모델 가중치 로드 시 map_location을 사용하여 CPU 또는 GPU로 로드 가능\n",
    "        state_dict = torch.load(model_conf['saved_path'], map_location=torch.device('cpu'))\n",
    "        # 만약 state_dict이 'model', 'state_dict' 등의 키를 가지고 있다면, 실제 가중치를 추출해야 할 수 있음\n",
    "        # 예: if 'state_dict' in state_dict: state_dict = state_dict['state_dict']\n",
    "        # 예: if 'model' in state_dict: state_dict = state_dict['model']\n",
    "        model.load_state_dict(state_dict)\n",
    "        print(f\"{model_conf['name']}의 가중치를 성공적으로 로드했습니다.\")\n",
    "    except Exception as e:\n",
    "        print(f\"오류: {model_conf['name']}의 가중치를 로드할 수 없습니다: {e}\")\n",
    "        # 가끔 DataParallel 등으로 저장된 모델은 키 앞에 'module.' 접두사가 붙을 수 있음\n",
    "        # 이 경우, 접두사 제거 후 다시 시도하는 로직 추가 가능\n",
    "        try:\n",
    "            print(\"키 앞에 'module.' 접두사가 있는지 확인 후 다시 로드 시도 중...\")\n",
    "            new_state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}\n",
    "            model.load_state_dict(new_state_dict)\n",
    "            print(f\"{model_conf['name']}의 가중치('module.' 제거 후)를 성공적으로 로드했습니다.\")\n",
    "        except Exception as e2:\n",
    "            print(f\"오류: 'module.' 접두사 제거 후에도 {model_conf['name']}의 가중치를 로드할 수 없습니다: {e2}\")\n",
    "            raise e # 원본 오류를 다시 발생시킴\n",
    "            \n",
    "    model = model.to(DEVICE)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def get_probabilities(model, loader, device, model_name=\"Model\"):\n",
    "    model.eval()\n",
    "    all_probs_list = []\n",
    "    all_original_filenames_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images_batch, filenames_batch in tqdm(loader, desc=f\"{model_name}으로 예측 중\"):\n",
    "            images_batch = images_batch.to(device)\n",
    "            outputs = model(images_batch)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            all_probs_list.append(probs.cpu())\n",
    "            all_original_filenames_list.extend(list(filenames_batch))\n",
    "\n",
    "    all_probs_tensor = torch.cat(all_probs_list, dim=0)\n",
    "    return all_probs_tensor.numpy(), all_original_filenames_list\n",
    "\n",
    "\n",
    "# --- 5. 모든 모델에 대한 추론 실행 및 앙상블 ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n앙상블 추론 프로세스 시작 중...\")\n",
    "    print(f\"사용 디바이스: {DEVICE}\")\n",
    "\n",
    "    all_model_probs = []\n",
    "    model_weights = []\n",
    "    image_paths_from_loader = None\n",
    "    processed_model_configs = [] # 성공적으로 처리된 모델의 설정을 저장\n",
    "\n",
    "    for i, model_conf in enumerate(MODEL_CONFIGS):\n",
    "        print(f\"\\n--- 모델 처리 중: {model_conf['name']} ---\")\n",
    "        try:\n",
    "            model = load_model_from_config(model_conf)\n",
    "            probs_np, current_paths = get_probabilities(model, test_loader, DEVICE, model_conf['name'])\n",
    "            all_model_probs.append(probs_np)\n",
    "            processed_model_configs.append(model_conf) # 성공 시 추가\n",
    "\n",
    "            if 'weight' in model_conf:\n",
    "                model_weights.append(model_conf['weight'])\n",
    "            else:\n",
    "                model_weights.append(1.0)\n",
    "\n",
    "            if image_paths_from_loader is None:\n",
    "                image_paths_from_loader = current_paths\n",
    "            elif image_paths_from_loader != current_paths:\n",
    "                print(\"경고: 모델 예측 간 이미지 경로 순서 불일치. 문제가 있을 수 있습니다.\")\n",
    "            \n",
    "            del model\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"오류: 모델 {model_conf['name']}의 가중치 파일을 찾을 수 없습니다: {e}. 이 모델은 앙상블에서 제외합니다.\")\n",
    "        except Exception as e:\n",
    "            print(f\"오류: 모델 {model_conf['name']} 처리 중 오류 발생: {e}. 이 모델은 앙상블에서 제외합니다.\")\n",
    "            # 이미 model_weights에 추가되었다면 제거 (오류 발생 전에 추가되므로)\n",
    "            # 이 부분은 현재 로직상 불필요 (오류 시 model_weights에 추가되지 않음)\n",
    "\n",
    "    if not all_model_probs:\n",
    "        print(\"오류: 성공적으로 처리된 모델이 없습니다. 앙상블을 생성할 수 없습니다. 종료합니다.\")\n",
    "        exit()\n",
    "\n",
    "    # --- 앙상블 예측 (소프트 보팅, 선택적 가중치 사용) ---\n",
    "    print(\"\\n소프트 보팅을 사용하여 예측 앙상블 중...\")\n",
    "\n",
    "    # 가중치 정규화 (이미 합이 1이 아니면 합이 1이 되도록)\n",
    "    # 성공적으로 처리된 모델들의 가중치만 사용\n",
    "    if sum(model_weights) == 0 and len(model_weights) > 0 : # 모든 가중치가 0인 극단적 경우 (하지만 모델은 있음)\n",
    "        print(\"경고: 총 가중치가 0이지만 처리된 모델이 있습니다. 동일한 가중치를 사용합니다.\")\n",
    "        normalized_weights = [1.0/len(all_model_probs)] * len(all_model_probs)\n",
    "    elif sum(model_weights) > 0 :\n",
    "        total_weight = sum(model_weights)\n",
    "        normalized_weights = [w / total_weight for w in model_weights]\n",
    "    else: # all_model_probs는 있지만 model_weights가 비어있거나 (이론상 불가능), 다른 문제\n",
    "        print(\"경고: 가중치 설정에 문제가 있습니다. 동일한 가중치를 사용합니다.\")\n",
    "        normalized_weights = [1.0/len(all_model_probs)] * len(all_model_probs)\n",
    "\n",
    "\n",
    "    ensembled_probs_sum = np.zeros_like(all_model_probs[0])\n",
    "    print(\"앙상블에 사용된 모델 및 가중치:\")\n",
    "    for i, probs_array in enumerate(all_model_probs):\n",
    "        # processed_model_configs[i]['name']을 사용하여 현재 모델의 이름 표시\n",
    "        print(f\"- 모델: {processed_model_configs[i]['name']}, 정규화된 가중치: {normalized_weights[i]:.4f}\")\n",
    "        ensembled_probs_sum += probs_array * normalized_weights[i]\n",
    "\n",
    "    final_predicted_indices = np.argmax(ensembled_probs_sum, axis=1)\n",
    "    ensembled_class_names = [CLASS_NAMES[idx] for idx in final_predicted_indices]\n",
    "\n",
    "    # --- 6. 제출 파일 생성 및 저장 ---\n",
    "    print(\"\\n제출 파일 준비 중...\")\n",
    "    original_test_df = pd.read_csv(TEST_CSV_PATH)\n",
    "\n",
    "    prediction_map = {\n",
    "        os.path.normpath(path): pred_label\n",
    "        for path, pred_label in zip(image_paths_from_loader, ensembled_class_names)\n",
    "    }\n",
    "\n",
    "    csv_img_path_col = test_dataset.img_path_column\n",
    "    mapped_predictions = original_test_df[csv_img_path_col].apply(\n",
    "        lambda x: prediction_map.get(os.path.normpath(x))\n",
    "    )\n",
    "\n",
    "    submission_df = pd.DataFrame()\n",
    "    # 원본 CSV의 'ID' 컬럼이 있다면 사용, 없다면 파일명에서 추출\n",
    "    if 'ID' in original_test_df.columns:\n",
    "        submission_df['ID'] = original_test_df['ID']\n",
    "    else:\n",
    "        submission_df['ID'] = original_test_df[csv_img_path_col].apply(\n",
    "            lambda x: os.path.splitext(os.path.basename(x))[0]\n",
    "        )\n",
    "    submission_df['rock_type'] = mapped_predictions\n",
    "\n",
    "    if submission_df['rock_type'].isnull().any():\n",
    "        num_null = submission_df['rock_type'].isnull().sum()\n",
    "        print(f\"경고: CSV의 {num_null}개 이미지가 예측을 받지 못했습니다. \"\n",
    "              \"누락된 이미지 파일 또는 매핑 중 오류 때문일 수 있습니다.\")\n",
    "        # submission_df['rock_type'].fillna('Etc', inplace=True) # 필요시 주석 해제\n",
    "        # print(f\"{num_null}개의 NaN을 'Etc'로 채웠습니다.\")\n",
    "\n",
    "    os.makedirs(OUTPUT_CSV_DIR, exist_ok=True)\n",
    "    \n",
    "    # 성공적으로 로드된 모델의 이름만 사용\n",
    "    ensemble_model_names_list = []\n",
    "    for cfg in processed_model_configs: # 성공적으로 처리된 모델 설정만 사용\n",
    "        cleaned_name = cfg[\"name\"].replace('_epoch', '').replace('_384','').replace('_f1_','_') # 이름 정제\n",
    "        # 특정 패턴 제거 (예: _0.xxxx 소수점 부분)\n",
    "        cleaned_name = '_'.join(cleaned_name.split('_')[:3]) # 이름이 너무 길어지는 것 방지 (예시)\n",
    "        ensemble_model_names_list.append(cleaned_name)\n",
    "\n",
    "    ensemble_model_names_str = \"_\".join(sorted(list(set(ensemble_model_names_list)))) # 중복 제거 및 정렬\n",
    "\n",
    "    if not ensemble_model_names_str:\n",
    "        ensemble_model_names_str = \"ENSEMBLE_FAILED\"\n",
    "    elif len(processed_model_configs) < len(MODEL_CONFIGS):\n",
    "         ensemble_model_names_str += \"_PARTIAL\"\n",
    "\n",
    "    output_csv_filename = f\"submission_ensemble_{ensemble_model_names_str}.csv\"\n",
    "    final_output_csv_path = os.path.join(OUTPUT_CSV_DIR, output_csv_filename)\n",
    "\n",
    "    submission_df.to_csv(final_output_csv_path, index=False)\n",
    "    print(f\"\\n앙상블 추론 완료. 예측 결과 저장 위치: {final_output_csv_path}\")\n",
    "    print(f\"제출 파일 샘플:\\n{submission_df.head()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a810bd20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timm 기반 변환 사용. 학습: Compose(\n",
      "    RandomResizedCropAndInterpolation(size=(384, 384), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic)\n",
      "    RandomHorizontalFlip(p=0.5)\n",
      "    ColorJitter(brightness=(0.6, 1.4), contrast=(0.6, 1.4), saturation=(0.6, 1.4), hue=None)\n",
      "    MaybeToTensor()\n",
      "    Normalize(mean=tensor([0.4850, 0.4560, 0.4060]), std=tensor([0.2290, 0.2240, 0.2250]))\n",
      ")\n",
      "timm 기반 변환 사용. 검증: Compose(\n",
      "    Resize(size=(384, 384), interpolation=bicubic, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(384, 384))\n",
      "    MaybeToTensor()\n",
      "    Normalize(mean=tensor([0.4850, 0.4560, 0.4060]), std=tensor([0.2290, 0.2240, 0.2250]))\n",
      ")\n",
      "OOF 예측 및 레이블 저장 위치: ./oof_predictions_imgfolder\n",
      "사용 디바이스: cuda\n",
      "ImageFolder가 인식한 클래스: ['Andesite', 'Basalt', 'Etc', 'Gneiss', 'Granite', 'Mud_Sandstone', 'Weathered_Rock']\n",
      "전체 학습 데이터 로드 완료 (샘플 수: 342015, 클래스 수: 7)\n",
      "전체 학습 데이터 레이블 및 클래스 이름 저장 완료.\n",
      "  train_labels.npy (형태: (342015,))\n",
      "  class_names_actual.txt (내용: ['Andesite', 'Basalt', 'Etc', 'Gneiss', 'Granite', 'Mud_Sandstone', 'Weathered_Rock'])\n",
      "\n",
      "--- 기본 모델 OOF 생성 시작: ConvNeXt_Large_384_ImageFolder ---\n",
      "\n",
      "  Fold 1/5 처리 중...\n",
      "  학습 데이터 수: 273612, 검증 데이터 수: 68403\n",
      "  Fold 1 모델 학습 시작 (에포크: 5)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 222\u001b[39m\n\u001b[32m    213\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Fold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m 모델 학습 시작 (에포크: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS_PER_FOLD\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS_PER_FOLD):\n\u001b[32m    215\u001b[39m     \u001b[38;5;66;03m# Warmup (간단한 선형 Warmup 예시)\u001b[39;00m\n\u001b[32m    216\u001b[39m     \u001b[38;5;66;03m# current_lr = LEARNING_RATE\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    219\u001b[39m     \u001b[38;5;66;03m# for param_group in optimizer.param_groups:\u001b[39;00m\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m#     param_group['lr'] = current_lr\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m222\u001b[39m     train_loss = \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mGRADIENT_CLIPPING\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# if scheduler:\u001b[39;00m\n\u001b[32m    224\u001b[39m     \u001b[38;5;66;03m#     scheduler.step()\u001b[39;00m\n\u001b[32m    225\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m    Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS_PER_FOLD\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;66;03m# , LR: {optimizer.param_groups[0]['lr']:.2e}\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 110\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(model, loader, criterion, optimizer, device, grad_clip_value)\u001b[39m\n\u001b[32m    108\u001b[39m outputs = model(images)\n\u001b[32m    109\u001b[39m loss = criterion(outputs, labels)\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m grad_clip_value:\n\u001b[32m    112\u001b[39m     torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip_value)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/env312_cuda124_torch260/lib/python3.12/site-packages/torch/_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/env312_cuda124_torch260/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/env312_cuda124_torch260/lib/python3.12/site-packages/torch/autograd/graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# generate_oof_predictions_imagefolder.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset # Dataset 제거 (ImageFolder 사용)\n",
    "from torchvision import transforms, datasets # datasets.ImageFolder 추가\n",
    "import timm\n",
    "# import pandas as pd # CSV 직접 사용 안 함\n",
    "from PIL import Image\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from timm.data import resolve_data_config, create_transform\n",
    "\n",
    "# --- 1. 설정 (Configuration) ---\n",
    "# --- 공통 파라미터 ---\n",
    "NUM_CLASSES = 7\n",
    "# CLASS_NAMES는 ImageFolder가 자동으로 폴더 순서대로 인식하므로,\n",
    "# ImageFolder 로드 후 실제 클래스 순서를 확인하고 일치시켜야 함\n",
    "# 또는, train_meta_models.py에서 CLASS_NAMES 순서에 맞게 레이블을 인코딩할 때 사용\n",
    "CLASS_NAMES_EXPECTED = ['Andesite', 'Basalt', 'Etc', 'Gneiss', 'Granite', 'Mud_Sandstone', 'Weathered_Rock']\n",
    "\n",
    "\n",
    "# --- 데이터 경로 설정 (ImageFolder용) ---\n",
    "TRAIN_DATA_ROOT_DIR = r\"/home/metaai2/workspace/limseunghwan/open\" # 학습/검증 데이터가 있는 최상위 디렉토리\n",
    "# TRAIN_DATA_DIR = os.path.join(TRAIN_DATA_ROOT_DIR, \"train\") # 이전에 사용된 변수명 유지 또는 변경\n",
    "# VAL_DATA_DIR = os.path.join(TRAIN_DATA_ROOT_DIR, \"val\") # 만약 train/val 분리된 경우\n",
    "\n",
    "# OOF는 전체 학습 데이터에 대해 생성하므로, train 폴더만 사용한다고 가정\n",
    "# 만약 train + val 데이터를 합쳐서 OOF를 만들고 싶다면, 두 ImageFolder를 ConcatDataset으로 합쳐야 함.\n",
    "# 여기서는 TRAIN_DATA_DIR만 사용한다고 가정\n",
    "TRAIN_IMAGE_FOLDER_PATH = os.path.join(TRAIN_DATA_ROOT_DIR, \"train\")\n",
    "\n",
    "\n",
    "OOF_OUTPUT_DIR = './oof_predictions_imgfolder' # OOF 예측 및 레이블 저장 디렉토리\n",
    "\n",
    "# --- 학습 및 OOF 생성 파라미터 ---\n",
    "N_SPLITS = 5\n",
    "RANDOM_STATE = 42\n",
    "BATCH_SIZE_TRAIN = 8 # 제공된 설정값 사용\n",
    "BATCH_SIZE_VAL = 16 # 검증은 더 크게 가능\n",
    "EPOCHS_PER_FOLD = 5 # 제공된 설정값 EPOCHS = 20을 각 Fold에 분배하거나, Fold마다 동일하게 적용\n",
    "                    # 여기서는 시연을 위해 5로 설정. 실제로는 더 길게 학습 필요\n",
    "LEARNING_RATE = 1e-5 # 제공된 설정값 BASE_LR 사용\n",
    "WEIGHT_DECAY = 1e-2\n",
    "WARMUP_EPOCHS = 5 # 이 예제에서는 Warmup 스케줄러는 생략 (필요시 추가)\n",
    "GRADIENT_CLIPPING = 1.0\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_WORKERS = os.cpu_count() // 2 if os.cpu_count() is not None else 4\n",
    "\n",
    "\n",
    "# --- 기본 모델 설정 (OOF 생성 대상 모델) ---\n",
    "# 제공해주신 설정에서는 단일 모델을 학습하는 형태였으나,\n",
    "# OOF는 여러 기본 모델에 대해 만들 수 있으므로 리스트 형태로 유지\n",
    "MODEL_CONFIGS = [\n",
    "    {\n",
    "        \"name\": \"ConvNeXt_Large_384_ImageFolder\", # OOF 파일명에 사용될 이름\n",
    "        \"architecture\": 'convnext_large.fb_in22k_ft_in1k_384',\n",
    "        \"img_size\": 384, # 모델 입력 이미지 크기\n",
    "        \"requires_img_size_arg\": False # ConvNeXt는 아키텍처 이름에 크기 포함\n",
    "    },\n",
    "    # 여기에 다른 기본 모델들을 추가할 수 있습니다.\n",
    "    # {\n",
    "    #     \"name\": \"MaxViT_XLarge_384_ImageFolder\",\n",
    "    #     \"architecture\": 'maxvit_xlarge_tf_384.in21k_ft_in1k',\n",
    "    #     \"img_size\": 384,\n",
    "    #     \"requires_img_size_arg\": True\n",
    "    # },\n",
    "]\n",
    "IMG_SIZE_FOR_LOADER = MODEL_CONFIGS[0][\"img_size\"] # 첫 번째 모델 기준으로 변환 설정\n",
    "\n",
    "# --- 2. 변환 정의 ---\n",
    "try:\n",
    "    temp_model_for_transform = timm.create_model(\n",
    "        MODEL_CONFIGS[0][\"architecture\"], pretrained=True, num_classes=NUM_CLASSES\n",
    "    )\n",
    "    data_config = resolve_data_config({}, model=temp_model_for_transform)\n",
    "    train_transform = create_transform(**data_config, is_training=True)\n",
    "    val_transform = create_transform(**data_config, is_training=False)\n",
    "    print(f\"timm 기반 변환 사용. 학습: {train_transform}\")\n",
    "    print(f\"timm 기반 변환 사용. 검증: {val_transform}\")\n",
    "    del temp_model_for_transform\n",
    "except Exception as e:\n",
    "    print(f\"timm 설정 가져오기 실패 ({e}). ImageNet 기본값 사용.\")\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((IMG_SIZE_FOR_LOADER, IMG_SIZE_FOR_LOADER)), # timm은 보통 RandomResizedCrop 사용\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1), # 가벼운 증강 추가\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize((IMG_SIZE_FOR_LOADER, IMG_SIZE_FOR_LOADER)), # Resize 후 CenterCrop 또는 바로 사용\n",
    "        # transforms.CenterCrop(IMG_SIZE_FOR_LOADER), # timm은 주로 Resize만 사용\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "# --- 3. 모델 학습 및 예측 함수 ---\n",
    "def train_one_epoch(model, loader, criterion, optimizer, device, grad_clip_value=None):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in tqdm(loader, desc=\"학습 중\", leave=False):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        if grad_clip_value:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip_value)\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "    return running_loss / len(loader.dataset)\n",
    "\n",
    "def get_oof_predictions_for_fold(model, loader, device):\n",
    "    model.eval()\n",
    "    fold_probs_list = []\n",
    "    with torch.no_grad():\n",
    "        # ImageFolder의 Subset을 사용할 때, Subset의 __getitem__은 (image, label)을 반환\n",
    "        for images, _ in tqdm(loader, desc=\"OOF 예측 중\", leave=False):\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            fold_probs_list.append(probs.cpu().numpy())\n",
    "    return np.concatenate(fold_probs_list, axis=0)\n",
    "\n",
    "# --- 4. OOF 예측 생성 메인 로직 ---\n",
    "if __name__ == \"__main__\":\n",
    "    os.makedirs(OOF_OUTPUT_DIR, exist_ok=True)\n",
    "    print(f\"OOF 예측 및 레이블 저장 위치: {OOF_OUTPUT_DIR}\")\n",
    "    print(f\"사용 디바이스: {DEVICE}\")\n",
    "\n",
    "    # 전체 학습 데이터셋 로드 (ImageFolder 사용, 변환은 나중에 적용)\n",
    "    # ImageFolder는 초기화 시 transform을 받지만, Subset에 직접 적용되지 않으므로,\n",
    "    # Subset을 만들고 DataLoader에 전달할 때 transform이 적용된 Dataset을 사용해야 함.\n",
    "    # 또는, fold마다 ImageFolder를 새로 생성하고 transform을 전달.\n",
    "    # 여기서는 먼저 전체 ImageFolder를 로드하여 레이블 정보와 클래스 순서를 얻음.\n",
    "    \n",
    "    # 임시 변환 없이 ImageFolder 로드 (클래스 및 레이블 정보 획득용)\n",
    "    temp_full_train_dataset = datasets.ImageFolder(root=TRAIN_IMAGE_FOLDER_PATH, transform=None)\n",
    "    \n",
    "    # ImageFolder가 인식한 클래스 순서 확인 및 CLASS_NAMES와 일치시키기\n",
    "    imagefolder_classes = temp_full_train_dataset.classes\n",
    "    print(f\"ImageFolder가 인식한 클래스: {imagefolder_classes}\")\n",
    "    if len(imagefolder_classes) != NUM_CLASSES:\n",
    "        raise ValueError(f\"ImageFolder 클래스 수({len(imagefolder_classes)})와 NUM_CLASSES({NUM_CLASSES}) 불일치\")\n",
    "    \n",
    "    # CLASS_NAMES_EXPECTED 순서에 맞게 ImageFolder의 레이블을 재매핑할 필요는 없음.\n",
    "    # ImageFolder는 자체적으로 0부터 시작하는 인덱스를 부여함.\n",
    "    # 생성된 OOF 파일과 함께 이 imagefolder_classes 순서를 기억해두거나,\n",
    "    # train_meta_models.py 에서 CLASS_NAMES_EXPECTED 를 기준으로 처리하면 됨.\n",
    "    # 여기서는 imagefolder_classes를 최종 클래스 이름으로 사용한다고 가정.\n",
    "    CLASS_NAMES_ACTUAL = imagefolder_classes # 실제 사용될 클래스 이름 순서\n",
    "\n",
    "    all_true_labels = np.array(temp_full_train_dataset.targets) # ImageFolder의 레이블 (정수 인덱스)\n",
    "    num_total_train_samples = len(all_true_labels)\n",
    "    print(f\"전체 학습 데이터 로드 완료 (샘플 수: {num_total_train_samples}, 클래스 수: {len(CLASS_NAMES_ACTUAL)})\")\n",
    "\n",
    "    # Stratified K-Fold 준비\n",
    "    skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "    # 전체 학습 데이터의 실제 레이블 저장 (train_meta_models.py 에서 사용)\n",
    "    np.save(os.path.join(OOF_OUTPUT_DIR, 'train_labels.npy'), all_true_labels)\n",
    "    # 클래스 이름 순서도 저장 (중요!)\n",
    "    with open(os.path.join(OOF_OUTPUT_DIR, 'class_names_actual.txt'), 'w') as f:\n",
    "        for class_name in CLASS_NAMES_ACTUAL:\n",
    "            f.write(f\"{class_name}\\n\")\n",
    "    print(f\"전체 학습 데이터 레이블 및 클래스 이름 저장 완료.\")\n",
    "    print(f\"  train_labels.npy (형태: {all_true_labels.shape})\")\n",
    "    print(f\"  class_names_actual.txt (내용: {CLASS_NAMES_ACTUAL})\")\n",
    "\n",
    "\n",
    "    # 각 기본 모델에 대해 OOF 예측 생성\n",
    "    for model_config in MODEL_CONFIGS:\n",
    "        print(f\"\\n--- 기본 모델 OOF 생성 시작: {model_config['name']} ---\")\n",
    "        model_oof_preds = np.zeros((num_total_train_samples, NUM_CLASSES))\n",
    "        \n",
    "        for fold, (train_idx, val_idx) in enumerate(skf.split(X=np.zeros(num_total_train_samples), y=all_true_labels)):\n",
    "            print(f\"\\n  Fold {fold+1}/{N_SPLITS} 처리 중...\")\n",
    "\n",
    "            # 현재 Fold의 학습 및 검증 Subset 생성 및 변환 적용\n",
    "            # 각 Fold마다 ImageFolder를 transform과 함께 새로 생성하고 Subset으로 나눔\n",
    "            # 이렇게 하면 Subset의 각 아이템에 transform이 올바르게 적용됨\n",
    "            train_fold_dataset = datasets.ImageFolder(root=TRAIN_IMAGE_FOLDER_PATH, transform=train_transform)\n",
    "            val_fold_dataset = datasets.ImageFolder(root=TRAIN_IMAGE_FOLDER_PATH, transform=val_transform)\n",
    "            \n",
    "            train_subset_for_loader = Subset(train_fold_dataset, train_idx)\n",
    "            val_subset_for_loader = Subset(val_fold_dataset, val_idx)\n",
    "\n",
    "            train_loader = DataLoader(train_subset_for_loader, batch_size=BATCH_SIZE_TRAIN, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "            val_loader = DataLoader(val_subset_for_loader, batch_size=BATCH_SIZE_VAL, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "            \n",
    "            print(f\"  학습 데이터 수: {len(train_subset_for_loader)}, 검증 데이터 수: {len(val_subset_for_loader)}\")\n",
    "\n",
    "            # 모델 초기화\n",
    "            model_args = {'pretrained': True, 'num_classes': NUM_CLASSES}\n",
    "            if model_config.get(\"requires_img_size_arg\", False):\n",
    "                 model_args['img_size'] = model_config['img_size']\n",
    "            \n",
    "            model = timm.create_model(model_config['architecture'], **model_args)\n",
    "            model = model.to(DEVICE)\n",
    "\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            # AdamW 옵티마이저 사용 (ConvNeXt 권장)\n",
    "            optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "            \n",
    "            # (선택 사항) 학습률 스케줄러 (예: CosineAnnealingLR with Warmup)\n",
    "            # scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=EPOCHS_PER_FOLD // 2, T_mult=1, eta_min=LEARNING_RATE * 0.01)\n",
    "            # Warmup 스케줄러는 직접 구현하거나 라이브러리 사용 필요\n",
    "\n",
    "            print(f\"  Fold {fold+1} 모델 학습 시작 (에포크: {EPOCHS_PER_FOLD})...\")\n",
    "            for epoch in range(EPOCHS_PER_FOLD):\n",
    "                # Warmup (간단한 선형 Warmup 예시)\n",
    "                # current_lr = LEARNING_RATE\n",
    "                # if epoch < WARMUP_EPOCHS:\n",
    "                #     current_lr = LEARNING_RATE * (epoch + 1) / WARMUP_EPOCHS\n",
    "                # for param_group in optimizer.param_groups:\n",
    "                #     param_group['lr'] = current_lr\n",
    "                \n",
    "                train_loss = train_one_epoch(model, train_loader, criterion, optimizer, DEVICE, GRADIENT_CLIPPING)\n",
    "                # if scheduler:\n",
    "                #     scheduler.step()\n",
    "                print(f\"    Epoch {epoch+1}/{EPOCHS_PER_FOLD}, Train Loss: {train_loss:.4f}\") # , LR: {optimizer.param_groups[0]['lr']:.2e}\n",
    "\n",
    "\n",
    "            fold_val_probs = get_oof_predictions_for_fold(model, val_loader, DEVICE)\n",
    "            model_oof_preds[val_idx] = fold_val_probs\n",
    "            \n",
    "            print(f\"  Fold {fold+1} OOF 예측 생성 완료. 형태: {fold_val_probs.shape}\")\n",
    "            del model, train_loader, val_loader\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        oof_filename = f\"{model_config['name'].replace(' ', '_')}_oof_probs.npy\"\n",
    "        oof_save_path = os.path.join(OOF_OUTPUT_DIR, oof_filename)\n",
    "        np.save(oof_save_path, model_oof_preds)\n",
    "        print(f\"\\n모델 {model_config['name']}의 전체 OOF 예측 저장 완료: {oof_save_path}, 형태: {model_oof_preds.shape}\")\n",
    "\n",
    "    print(\"\\n모든 기본 모델에 대한 OOF 예측 생성 완료.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346efbe1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env312_cuda124_torch260",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
