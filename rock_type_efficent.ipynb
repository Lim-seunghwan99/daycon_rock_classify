{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50902f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: tf_efficientnetv2_m.in21k_ft_in1k\n",
      "Model classifier replaced for 7 classes.\n",
      "Using timm's default validation transform for the new model.\n",
      "Timm default input size for tf_efficientnetv2_m.in21k_ft_in1k: (3, 384, 384)\n",
      "Overriding to use specified IMG_SIZE: 384 for validation transform consistency.\n",
      "Loading datasets from: /home/metaai2/workspace/limseunghwan/open/train and /home/metaai2/workspace/limseunghwan/open/val\n",
      "Found 342015 training images and 38005 validation images.\n",
      "Classes: ['Andesite', 'Basalt', 'Etc', 'Gneiss', 'Granite', 'Mud_Sandstone', 'Weathered_Rock']\n",
      "Calculated Focal Loss alpha (normalized): [0.36379087 0.59434706 1.         0.21558282 0.17148152 0.17810482\n",
      " 0.4287038 ]\n",
      "Using Focal Loss with calculated alpha.\n",
      "\n",
      "Starting training process...\n",
      "학습 시작: 총 20 에폭, Device: cuda\n",
      "Top-5 모델 저장 디렉토리: ./saved_models_efficientnetv2_m\n",
      "평가 기준: Validation Macro F1 Score\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 334\u001b[39m\n\u001b[32m    331\u001b[39m \u001b[38;5;66;03m# 모델 파일 이름에 사용될 기본 이름 생성\u001b[39;00m\n\u001b[32m    332\u001b[39m model_name_base = MODEL_NAME.split(\u001b[33m'\u001b[39m\u001b[33m/\u001b[39m\u001b[33m'\u001b[39m)[-\u001b[32m1\u001b[39m].split(\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m)[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33m/\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m MODEL_NAME \u001b[38;5;28;01melse\u001b[39;00m MODEL_NAME.split(\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m)[\u001b[32m0\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m history = \u001b[43mtrain_and_validate_best_f1\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    337\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    338\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    339\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    340\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    342\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mNUM_CLASSES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    343\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mSAVE_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    344\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name_base\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_name_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    345\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    346\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgradient_clipping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mGRADIENT_CLIPPING\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwarmup_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mWARMUP_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbase_lr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBASE_LR\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTraining finished.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    353\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m history[\u001b[33m'\u001b[39m\u001b[33mval_macro_f1_scores\u001b[39m\u001b[33m'\u001b[39m]:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 117\u001b[39m, in \u001b[36mtrain_and_validate_best_f1\u001b[39m\u001b[34m(model, train_loader, val_loader, optimizer, criterion, epochs, device, num_classes, save_dir, model_name_base, top_k, gradient_clipping, lr_scheduler, warmup_epochs, base_lr)\u001b[39m\n\u001b[32m    115\u001b[39m         torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clipping)\n\u001b[32m    116\u001b[39m     optimizer.step()\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     running_train_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m     train_pbar.set_postfix(loss=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss.item()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    119\u001b[39m epoch_train_loss = running_train_loss / \u001b[38;5;28mlen\u001b[39m(train_loader)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# efficientnetv2.py \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms # torchvision.transforms 사용\n",
    "import torch.nn.functional as F # FocalLoss 등에서 필요\n",
    "import timm\n",
    "import torchmetrics\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import os\n",
    "import numpy as np # 필요시 사용\n",
    "\n",
    "# --- 1. Focal Loss 클래스 정의 (이전 코드에서 복사) ---\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2., reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            if isinstance(self.alpha, (float, int)):\n",
    "                 alpha = torch.tensor([self.alpha] * inputs.shape[1], device=inputs.device)\n",
    "            elif isinstance(self.alpha, list):\n",
    "                 alpha = torch.tensor(self.alpha, device=inputs.device, dtype=torch.float32)\n",
    "            elif torch.is_tensor(self.alpha):\n",
    "                 alpha = self.alpha.to(device=inputs.device, dtype=torch.float32)\n",
    "            else:\n",
    "                 raise TypeError(\"alpha must be float, list or torch.Tensor\")\n",
    "\n",
    "            if alpha.shape[0] != inputs.shape[1]:\n",
    "                 raise ValueError(f\"alpha size {alpha.shape[0]} does not match C {inputs.shape[1]}\")\n",
    "\n",
    "            alpha_t = alpha.gather(0, targets)\n",
    "            focal_loss = alpha_t * focal_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else: # 'none'\n",
    "            return focal_loss\n",
    "\n",
    "# --- 2. 학습 및 검증 함수 정의 (이전 코드에서 복사) ---\n",
    "def train_and_validate_best_f1(model: nn.Module,\n",
    "                               train_loader: DataLoader,\n",
    "                               val_loader: DataLoader,\n",
    "                               optimizer: optim.Optimizer, # torch.optim 임포트 사용\n",
    "                               criterion: nn.Module, # Loss function\n",
    "                               epochs: int,\n",
    "                               device: torch.device,\n",
    "                               num_classes: int,\n",
    "                               save_dir: str, # 디렉토리로 변경\n",
    "                               model_name_base: str, # 모델 파일 이름용\n",
    "                               top_k: int = 5, # 저장할 상위 모델 개수 (기본값 5)\n",
    "                               gradient_clipping: float = None,\n",
    "                               lr_scheduler = None,\n",
    "                               warmup_epochs: int = 0,\n",
    "                               base_lr: float = 1e-5\n",
    "                              ):\n",
    "\n",
    "    history = {'train_losses': [], 'val_losses': [], 'val_macro_f1_scores': []}\n",
    "    max_val_f1 = 0.0\n",
    "    best_epoch = -1\n",
    "    top_k_checkpoints = []\n",
    "\n",
    "    f1_metric = torchmetrics.F1Score(task='multiclass', num_classes=num_classes, average='macro').to(device)\n",
    "    patience = 5 # 조기 종료를 위한 인내 에폭 수\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"학습 시작: 총 {epochs} 에폭, Device: {device}\")\n",
    "    print(f\"Top-{top_k} 모델 저장 디렉토리: {save_dir}\")\n",
    "    print(f\"평가 기준: Validation Macro F1 Score\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "        if epoch < warmup_epochs:\n",
    "            warmup_factor = (epoch + 1) / warmup_epochs\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = base_lr * warmup_factor\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "        elif lr_scheduler is not None:\n",
    "             if epoch == warmup_epochs: # 첫 스케줄러 스텝 전 LR을 base_lr로 설정\n",
    "                 for param_group in optimizer.param_groups:\n",
    "                     param_group['lr'] = base_lr\n",
    "                 current_lr = optimizer.param_groups[0]['lr'] # 업데이트된 LR 반영\n",
    "                 # Warmup 종료 후 첫 스케줄러 스텝 (CosineAnnealingLR 등은 epoch 기준으로 동작)\n",
    "                 # scheduler.step()은 epoch 마다 호출되므로, warmup 후 첫 epoch에서 base_lr을 사용하도록 함\n",
    "                 # 이미 optimizer의 lr이 base_lr로 설정되었으므로, scheduler.step()을 바로 호출해도 됨\n",
    "             lr_scheduler.step() # warmup이 끝난 후 매 에폭마다 스케줄러 호출\n",
    "             current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "        train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Train] LR: {current_lr:.1e}\", leave=False)\n",
    "        for images, labels in train_pbar:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            if gradient_clipping is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clipping)\n",
    "            optimizer.step()\n",
    "            running_train_loss += loss.item()\n",
    "            train_pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "        epoch_train_loss = running_train_loss / len(train_loader)\n",
    "        history['train_losses'].append(epoch_train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        running_val_loss = 0.0\n",
    "        f1_metric.reset()\n",
    "        val_pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} [Val] \", leave=False)\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_pbar:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                running_val_loss += loss.item()\n",
    "                f1_metric.update(outputs, labels)\n",
    "                val_pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "        epoch_val_loss = running_val_loss / len(val_loader)\n",
    "        history['val_losses'].append(epoch_val_loss)\n",
    "        epoch_val_f1 = f1_metric.compute().item()\n",
    "        history['val_macro_f1_scores'].append(epoch_val_f1)\n",
    "\n",
    "        epoch_end_time = time.time()\n",
    "        epoch_duration = epoch_end_time - epoch_start_time\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] ({epoch_duration:.2f}s) - \"\n",
    "              f\"Train Loss: {epoch_train_loss:.4f}, \"\n",
    "              f\"Val Loss: {epoch_val_loss:.4f}, \"\n",
    "              f\"Val Macro F1: {epoch_val_f1:.4f}\")\n",
    "\n",
    "        is_top_k = len(top_k_checkpoints) < top_k or epoch_val_f1 > top_k_checkpoints[-1][0]\n",
    "\n",
    "        if is_top_k:\n",
    "            checkpoint_filename = f\"{model_name_base}_epoch{epoch+1}_f1_{epoch_val_f1:.4f}.pth\"\n",
    "            checkpoint_path = os.path.join(save_dir, checkpoint_filename)\n",
    "            try:\n",
    "                torch.save(model.state_dict(), checkpoint_path)\n",
    "                print(f\"  Checkpoint saved to {checkpoint_path}\")\n",
    "                top_k_checkpoints.append((epoch_val_f1, checkpoint_path))\n",
    "                top_k_checkpoints.sort(key=lambda x: x[0], reverse=True)\n",
    "                if len(top_k_checkpoints) > top_k:\n",
    "                    score_to_remove, path_to_remove = top_k_checkpoints.pop()\n",
    "                    print(f\"  Removing checkpoint {os.path.basename(path_to_remove)} (score: {score_to_remove:.4f}) as it's no longer in top-{top_k}\")\n",
    "                    if os.path.exists(path_to_remove):\n",
    "                        try:\n",
    "                            os.remove(path_to_remove)\n",
    "                        except Exception as e_rem:\n",
    "                            print(f\"    Error removing file {path_to_remove}: {e_rem}\")\n",
    "                    else:\n",
    "                        print(f\"    Warning: File to remove not found: {path_to_remove}\")\n",
    "            except Exception as e_save:\n",
    "                print(f\"  Error saving checkpoint: {e_save}\")\n",
    "\n",
    "        if epoch_val_f1 > max_val_f1:\n",
    "            print(f\"  Validation Macro F1 improved ({max_val_f1:.4f} --> {epoch_val_f1:.4f}).\")\n",
    "            max_val_f1 = epoch_val_f1\n",
    "            best_epoch = epoch\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            print(f\"  Validation Macro F1 did not improve from the best ({max_val_f1:.4f}). ({epochs_no_improve}/{patience})\")\n",
    "\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch+1} after {patience} epochs without improvement from the best F1 score.\")\n",
    "            break\n",
    "\n",
    "    print(f\"\\n학습 완료.\")\n",
    "    print(f\"최종 Top-{top_k} 모델 성능 및 경로:\")\n",
    "    if not top_k_checkpoints:\n",
    "        print(\"  No models were saved.\")\n",
    "    else:\n",
    "        for i, (score, path) in enumerate(top_k_checkpoints):\n",
    "            print(f\"  Top {i+1}: Score={score:.4f}, Path={path}\")\n",
    "    print(f\"\\nOverall Best Epoch: {best_epoch+1 if best_epoch != -1 else 'N/A'}, Overall Best Validation Macro F1: {max_val_f1:.4f}\")\n",
    "\n",
    "    return history\n",
    "\n",
    "# --- 3. 설정 변수 정의 ---\n",
    "MODEL_NAME = 'tf_efficientnetv2_m.in21k_ft_in1k' # <<< 변경된 모델 이름\n",
    "NUM_CLASSES = 7\n",
    "IMG_SIZE = 384 # EfficientNetV2-M은 384 또는 480 에서 좋은 성능을 보임. 일단 384 유지.\n",
    "               # 더 높은 해상도(예: 480)를 사용하면 성능이 향상될 수 있지만 메모리 요구량 증가\n",
    "\n",
    "# --- 데이터 경로 설정 ---\n",
    "# TODO: 실제 데이터 경로로 수정하세요\n",
    "TRAIN_DATA_DIR = r\"/home/metaai2/workspace/limseunghwan/open/train\" # 예시 경로\n",
    "VAL_DATA_DIR = r\"/home/metaai2/workspace/limseunghwan/open/val\"     # 예시 경로\n",
    "\n",
    "# --- 학습 하이퍼파라미터 ---\n",
    "EPOCHS = 20 # 에폭 수 (필요에 따라 조절)\n",
    "BATCH_SIZE = 8 # 배치 크기 (GPU 메모리에 맞게 조절, EfficientNetV2-M은 ConvNeXt-Large보다 메모리를 적게 사용할 수도, 또는 더 많이 사용할 수도 있음. 테스트 필요)\n",
    "BASE_LR = 1e-5 # 기본 학습률 (Fine-tuning에 적합한 값으로 시작, 모델에 따라 튜닝 필요)\n",
    "WEIGHT_DECAY = 1e-2 # 가중치 감쇠 (AdamW와 함께 사용)\n",
    "WARMUP_EPOCHS = 5   # Warmup 에폭 수\n",
    "GRADIENT_CLIPPING = 1.0 # Gradient Clipping 값 (사용하지 않으려면 None)\n",
    "\n",
    "# --- 시스템 설정 ---\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_WORKERS = os.cpu_count() // 2 if os.cpu_count() else 4 # 사용 가능한 CPU 코어의 절반 정도 (조절 가능)\n",
    "\n",
    "# --- 저장 경로 설정 ---\n",
    "SAVE_DIR = './saved_models_efficientnetv2_m' # 모델 저장 디렉토리 (모델별로 구분)\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# --- 4. 모델 로드 및 수정 ---\n",
    "print(f\"Loading model: {MODEL_NAME}\")\n",
    "model = timm.create_model(MODEL_NAME, pretrained=True)\n",
    "\n",
    "# 모델의 분류기(head) 부분을 새로운 클래스 수에 맞게 교체\n",
    "# EfficientNetV2의 경우에도 reset_classifier가 잘 동작합니다.\n",
    "# 내부적으로 classifier 라는 이름의 attribute를 찾아 교체합니다.\n",
    "model.reset_classifier(num_classes=NUM_CLASSES)\n",
    "print(f\"Model classifier replaced for {NUM_CLASSES} classes.\")\n",
    "\n",
    "# 모델을 지정된 장치로 이동\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "# --- 5. 데이터 변환 정의 ---\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(size=(IMG_SIZE, IMG_SIZE), scale=(0.5, 1.0), ratio=(0.75, 1.3333), interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=15, interpolation=transforms.InterpolationMode.BILINEAR, fill=0),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # ImageNet 기본값 사용\n",
    "])\n",
    "\n",
    "try:\n",
    "    config = timm.data.resolve_data_config({}, model=model) # 새 모델에 맞는 config 로드\n",
    "    val_transform = timm.data.create_transform(**config, is_training=False)\n",
    "    print(\"Using timm's default validation transform for the new model.\")\n",
    "    # timm transform의 input_size가 IMG_SIZE와 다를 수 있습니다.\n",
    "    # 만약 timm 기본 input_size를 사용하고 싶다면, IMG_SIZE를 config['input_size'][-1]로 설정하세요.\n",
    "    # 여기서는 사용자가 지정한 IMG_SIZE를 따르도록 val_transform을 덮어쓸 수 있습니다 (아래 코드 참고).\n",
    "    # 또는, val_transform을 그대로 사용하고, IMG_SIZE는 참고용으로만 둡니다.\n",
    "    # 여기서는 명시적으로 IMG_SIZE에 맞추겠습니다.\n",
    "    # val_transform.transforms[0] = transforms.Resize(IMG_SIZE, interpolation=transforms.InterpolationMode.BICUBIC)\n",
    "    # val_transform.transforms[1] = transforms.CenterCrop(IMG_SIZE)\n",
    "    # 위 방식보다 아래처럼 명시적 재정의가 더 확실할 수 있습니다.\n",
    "    print(f\"Timm default input size for {MODEL_NAME}: {config['input_size']}\")\n",
    "    print(f\"Overriding to use specified IMG_SIZE: {IMG_SIZE} for validation transform consistency.\")\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize(IMG_SIZE, interpolation=transforms.InterpolationMode.BICUBIC), # 또는 config['input_size'][-2:]\n",
    "        transforms.CenterCrop(IMG_SIZE), # 또는 config['input_size'][-1]\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=config['mean'], std=config['std']) # 모델별 mean/std 사용\n",
    "    ])\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Failed to get timm config ({e}), defining validation transform manually.\")\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize(IMG_SIZE, interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "        transforms.CenterCrop(IMG_SIZE),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "# --- 6. 데이터셋 및 데이터 로더 준비 ---\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "print(f\"Loading datasets from: {TRAIN_DATA_DIR} and {VAL_DATA_DIR}\")\n",
    "try:\n",
    "    train_dataset = ImageFolder(root=TRAIN_DATA_DIR, transform=train_transform)\n",
    "    val_dataset = ImageFolder(root=VAL_DATA_DIR, transform=val_transform)\n",
    "\n",
    "    print(f\"Found {len(train_dataset)} training images and {len(val_dataset)} validation images.\")\n",
    "    print(f\"Classes: {train_dataset.classes}\")\n",
    "\n",
    "    class_counts = np.bincount([s[1] for s in train_dataset.samples])\n",
    "    focal_loss_alpha = None # 기본값\n",
    "    if len(class_counts) == NUM_CLASSES:\n",
    "        total_samples = sum(class_counts)\n",
    "        # 0으로 나누기 방지 및 가중치 계산 (빈도 역수에 비례)\n",
    "        class_weights_raw = [total_samples / count if count > 0 else 0 for count in class_counts]\n",
    "        \n",
    "        # 가중치 정규화 (최대 가중치가 1이 되도록)\n",
    "        max_weight = max(class_weights_raw) if any(w > 0 for w in class_weights_raw) else 1\n",
    "        if max_weight > 0: # 모든 클래스가 0개가 아닌 경우\n",
    "            class_weights_normalized = [w / max_weight for w in class_weights_raw]\n",
    "            focal_loss_alpha = torch.tensor(class_weights_normalized, device=DEVICE, dtype=torch.float32)\n",
    "            print(f\"Calculated Focal Loss alpha (normalized): {focal_loss_alpha.cpu().numpy()}\")\n",
    "        else: # 모든 클래스 카운트가 0인 극단적 경우 (실제로는 거의 발생 안함)\n",
    "            print(\"Warning: All class counts are zero. Cannot calculate Focal Loss alpha.\")\n",
    "    else:\n",
    "        print(f\"Warning: Number of found classes ({len(class_counts)}) in dataset does not match NUM_CLASSES ({NUM_CLASSES}). Focal Loss alpha set to None.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Data directory not found. Please check TRAIN_DATA_DIR and VAL_DATA_DIR.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")\n",
    "    exit()\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "# --- 7. 손실 함수, 옵티마이저, 스케줄러 정의 ---\n",
    "if focal_loss_alpha is not None:\n",
    "    criterion = FocalLoss(alpha=focal_loss_alpha, gamma=2.0).to(DEVICE)\n",
    "    print(\"Using Focal Loss with calculated alpha.\")\n",
    "else:\n",
    "    criterion = FocalLoss(gamma=2.0).to(DEVICE) # Alpha 없이 Focal Loss 사용\n",
    "    # criterion = nn.CrossEntropyLoss().to(DEVICE) # 또는 CrossEntropy 사용\n",
    "    print(\"Using Focal Loss without alpha (or CrossEntropyLoss if preferred).\")\n",
    "\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=BASE_LR, weight_decay=WEIGHT_DECAY)\n",
    "# Warmup 후 T_max는 (총 에폭 - warmup 에폭)\n",
    "lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS - WARMUP_EPOCHS, eta_min=BASE_LR * 0.01 if EPOCHS > WARMUP_EPOCHS else BASE_LR)\n",
    "\n",
    "\n",
    "# --- 8. 학습 및 검증 실행 ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\nStarting training process...\")\n",
    "    # 모델 파일 이름에 사용될 기본 이름 생성\n",
    "    model_name_base = MODEL_NAME.split('/')[-1].split('.')[0] if '/' in MODEL_NAME else MODEL_NAME.split('.')[0]\n",
    "    \n",
    "    history = train_and_validate_best_f1(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        epochs=EPOCHS,\n",
    "        device=DEVICE,\n",
    "        num_classes=NUM_CLASSES,\n",
    "        save_dir=SAVE_DIR,\n",
    "        model_name_base=model_name_base,\n",
    "        top_k=5,\n",
    "        gradient_clipping=GRADIENT_CLIPPING,\n",
    "        lr_scheduler=lr_scheduler,\n",
    "        warmup_epochs=WARMUP_EPOCHS,\n",
    "        base_lr=BASE_LR\n",
    "    )\n",
    "\n",
    "    print(\"\\nTraining finished.\")\n",
    "    if history['val_macro_f1_scores']:\n",
    "        print(\"Training History Summary:\")\n",
    "        best_f1_overall = max(history['val_macro_f1_scores'])\n",
    "        best_epoch_overall = history['val_macro_f1_scores'].index(best_f1_overall) + 1\n",
    "        print(f\"  Overall Best Validation Macro F1 in history: {best_f1_overall:.4f}\")\n",
    "        print(f\"  Achieved at Epoch: {best_epoch_overall}\")\n",
    "    else:\n",
    "        print(\"No validation scores recorded in history.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83a3b18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env312_cuda124_torch260",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
